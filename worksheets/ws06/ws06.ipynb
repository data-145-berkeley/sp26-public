{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e0aded-fdf3-4f59-9537-3ad5348b8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks2_plot(x_interval, sample1, sample2):\n",
    "    xmin = x_interval[0]\n",
    "    xmax = x_interval[1]\n",
    "    n=len(sample1)\n",
    "    sorted_sample1 = np.sort(sample1)\n",
    "    plt.step(sorted_sample1, np.arange(1, n+1)/n, where='post', lw=2, color='red', label='ECDF of Sample X')\n",
    "    if sorted_sample1.item(0) > xmin:\n",
    "        plt.plot((xmin, sorted_sample1.item(0), sorted_sample1.item(0)), (0, 0, 1/n), lw=2, color='red')\n",
    "    if sorted_sample1.item(n-1) < xmax:\n",
    "        plt.plot((sorted_sample1.item(n-1), xmax), (1, 1), lw=2, color='red')\n",
    "    m = len(sample2)\n",
    "    sorted_sample2 = np.sort(sample2)\n",
    "    plt.step(sorted_sample2, np.arange(1, m+1)/m, where='post', lw=2, color='darkblue', label='ECDF of Sample Y')\n",
    "    if sorted_sample2.item(0) > xmin:\n",
    "        plt.plot((xmin, sorted_sample2.item(0), sorted_sample2.item(0)), (0, 0, 1/m), lw=2, color='darkblue')\n",
    "    if sorted_sample2.item(m-1) < xmax:\n",
    "        plt.plot((sorted_sample2.item(m-1), xmax), (1, 1), lw=2, color='darkblue')\n",
    "    plt.legend()\n",
    "    plt.title(f'n = {n}, m = {m}', size=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Worksheet 6 #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem1",
   "metadata": {},
   "source": [
    "## 1. Is it Exponential?\n",
    "\n",
    "In Lecture 1 we used the exponential as a model for the interarrival times of California earthquakes of magnitude at least 4. If the earthquakes arrive like a Poisson process, the interarrival times are exponential. Based on the histogram of the observed distribution of those times, and some basic checking of means and variances, the exponential model seemed reasonable.\n",
    "\n",
    "And then we said that later in the term we'd develop a method for testing whether data are like i.i.d. draws from an exponential distribution.\n",
    "\n",
    "That time has come. In this exercise you will read in the data by running the cells below, and then perform a Kolmogorov-Smirnov test of whether the interarrival times are like an i.i.d. sample from an exponential distribution with unknown rate $\\lambda$.\n",
    "\n",
    "The code is taken from Lecture 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa83e9-509c-4094-99de-fdddfbf945f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the declustered earthquake data (produced by usgs_gk_ca_mainshocks.ipynb)\n",
    "earthquakes = pd.read_csv('california_earthquakes_declustered.csv')\n",
    "earthquakes['time'] = pd.to_datetime(earthquakes['time'], format='ISO8601')\n",
    "\n",
    "print(f\"Total earthquakes in dataset: {len(earthquakes)}\")\n",
    "print(f\"  - Mainshocks: {earthquakes['is_mainshock'].sum()}\")\n",
    "print(f\"  - Dependent: {(~earthquakes['is_mainshock']).sum()}\")\n",
    "print(f\"Date range: {earthquakes['time'].min().date()} to {earthquakes['time'].max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae21dc-4db8-42b1-bcf2-ca4a8df3e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to mainshocks with M >= 4.0\n",
    "mag_threshold = 4.0\n",
    "mainshocks = earthquakes[(earthquakes['is_mainshock']) & (earthquakes['mag'] >= mag_threshold)].copy()\n",
    "mainshocks = mainshocks.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "print(f\"Mainshocks with M >= {mag_threshold}: {len(mainshocks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86144620-4656-4759-ba94-420e96eb993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute interarrival times (in days)\n",
    "interarrivals = mainshocks['time'].diff().dt.total_seconds() / (60 * 60 * 24)\n",
    "interarrivals = interarrivals.dropna().values\n",
    "\n",
    "print(f\"Number of interarrival times: {len(interarrivals)}\")\n",
    "print(f\"Mean interarrival time: {np.mean(interarrivals):.2f} days\")\n",
    "print(f\"Median interarrival time: {np.median(interarrivals):.2f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbf4e0-6a06-4f34-be3f-a066ad6576a8",
   "metadata": {},
   "source": [
    "The array `interarrivals` is the sample. The null hypothesis is that it looks like i.i.d. draws from an exponential distribution with an unknown rate $\\lambda$. \n",
    "\n",
    "**(a)** Find $\\hat{\\lambda}$, the MLE of $\\lambda$. Refer to Lecture 1 for this if necessary.\n",
    "\n",
    "**(b)** Find the Kolmogorov-Smirnov distance between the empirical cdf of the sample and the cdf of the exponential distribution with parameter $\\hat{\\lambda}$. You can use [`kstest`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html) for this. See the last code cell in Section 1 of Lecture 11 to see how to extract just the test statistic.\n",
    "\n",
    "**(c)** Follow the parametric bootstrap process outlined in Section 4 of Lecture 11 to simulate the null distribution of the test statistic. That is, repeat the following 10,000 times and draw the empirical histogram of the results.\n",
    "- The simulation is under the null hypothesis, so generate a new sample of the appropriate size from $F_{\\hat{\\theta}}$.\n",
    "- Let $\\hat{\\theta}^*$ be the MLE of $\\theta$ based on this sample, and let the corresponding $F_{\\hat{\\theta}^*}$ be your new \"true\" distribution.\n",
    "- Calculate the KS distance between the ecdf of the new sample and the new \"true\" $F_{\\hat{\\theta}^*}$.\n",
    "\n",
    "**(d)** Approximately what is the $p$-value of your test? Do you still think the exponential model looks reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae31ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use as many cells as you need\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6722a995-4d1a-44b3-890f-f61a7ef2d93a",
   "metadata": {},
   "source": [
    "## 2. Two-Sample Kolmogorov-Smirnov Test and Stochastic Ordering\n",
    "The dataset `births` comes from a random sample of mother-newborn pairs. Each row contains data on one such pair. The column `Birth Weight` contains the newborn's birth weight in ounces. The column `Maternal Smoker` has Boolean values for whether or not the mother smoked during pregnancy.\n",
    "\n",
    "In what way is maternal smoking during pregnancy associated with the birth weight of their baby? A step in addressing this question is to see whether there's an association in the first place. Let \"smoker\" be short for for \"mother who smoked during pregnancy,\" and define \"nonsmoker\" analogously. You can assume that the smokers and nonsmokers were drawn independently of each other. You can also assume that within each of the two categories, the pairs are i.i.d.\n",
    "\n",
    "**(a)** Complete the cell below to plot the empirical cdfs of the two samples. Each of `smoker_sample` and `nonsmoker_sample` should be an array of birth weights of the appropriate sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf904d-1b59-4fbe-b118-31b6a70d6dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to 2a\n",
    "births = pd.read_csv('births.csv')\n",
    "\n",
    "smoker_sample = ...\n",
    "nonsmoker_sample = ...\n",
    "\n",
    "ks2_plot([50, 200], smoker_sample, nonsmoker_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bd46d1-bfa7-4239-8c25-107b3579672f",
   "metadata": {},
   "source": [
    "**(b)** Let $F$ and $G$ be the underlying cdfs of a birth weight in the smoker population and non-smoker population respectively. Run the cell below to perform the two-sample Kolmogorov-Smirnov test of the null hypothesis $H_0: F = G$. What does the test conclude, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfbbcae-2184-43ab-b2ac-9737add8ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ks_2samp(smoker_sample, nonsmoker_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767fe039-8bb4-4b64-8540-b85422d0a33e",
   "metadata": {},
   "source": [
    "**(c)** Simulate the null distribution of the statistic in the following steps, and say whether the distribution is consistent with the $p$-value in Part **b**.\n",
    "- Under the null hypothesis, the birth weights in the entire data set are i.i.d. It doesn't matter what label each row has. Use this idea to define a function `one_ksd()` that takes no arguments and returns one simulated value of the Kolmogorov-Smirnov distance under the null hypothesis. You should shuffle the `Maternal Smoker` column (see documentation for `np.random.permutation`) and then split the relabeled sample as you did in Part **a**. To get just the value of the statistic, use `ks_2samp(sample1, sample2).statistic`.\n",
    "- Generate 10,000 simulated values of the statistic, calling `one_ksd()` each time.\n",
    "- Draw the histogram of the 10,000 simulated values.\n",
    "- Examine the range of simulated values and explain why the $p$-value in Part **b** is or is not consistent with the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf96d10-51a9-4202-afdb-2cdb32f24c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to 2c (use as many lines of code as you need)\n",
    "\n",
    "def one_ksd():\n",
    "    ...    \n",
    "\n",
    "ksds = np.array([])\n",
    "...\n",
    "\n",
    "plt.hist(ksds, color='tab:blue', edgecolor='w', bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8f1fe-9955-42d2-b42a-9d62e93f50d6",
   "metadata": {},
   "source": [
    "**(d)** Use the plot in Part **a** to say which of the two you think is bigger: $F(120)$ or $G(120)$? Explain, keeping in mind that the plot shows neither $F$ nor $G$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1384e-3107-4d28-950d-ff762afab601",
   "metadata": {},
   "source": [
    "**(e)** Use the empirical cdfs to say which group has babies with lower weights, and explain your reasoning. \n",
    "\n",
    "The terminology for what you are observing is *stochastic ordering*. A distribution with cdf $F_1$ is said to be *stochastically smaller* than a distribution with cdf $F_2$ if $F_1(x) \\ge F_2(x)$ for all $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem2-title",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## 3. Neyman–Pearson Examples\n",
    "\n",
    "Recall the **Neyman–Pearson Lemma**: to test $H_0: \\theta = \\theta_0$ versus $H_1: \\theta = \\theta_1$ at level $\\alpha$, the most powerful test rejects for large values of the **likelihood ratio**\n",
    "\n",
    "$$\\text{LR}(\\mathbf{X}) = \\frac{f_{\\theta_1}(\\mathbf{X})}{f_{\\theta_0}(\\mathbf{X})}.$$\n",
    "\n",
    "That is, the NP test rejects when $\\text{LR}(\\mathbf{X}) > c$ for a cutoff $c$ chosen so that $P_{\\theta_0}(\\text{LR}(\\mathbf{X}) > c) = \\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem2a",
   "metadata": {},
   "source": [
    "**(a)** Suppose the NP test rejects $H_0$ when $\\text{LR}(\\mathbf{X}) > c$, and let $\\varphi$ be a strictly increasing function. Show that there exists a cutoff $c'$ such that rejecting when $\\varphi(\\text{LR}(\\mathbf{X})) > c'$ gives exactly the same test.\n",
    "\n",
    "Use this to conclude that the NP test can equivalently be written as: reject for large values of any statistic $T(\\mathbf{X})$ that is a strictly increasing function of $\\text{LR}(\\mathbf{X})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem2b",
   "metadata": {},
   "source": [
    "**(b)** Let $X_1, X_2, \\ldots, X_n$ be i.i.d. $\\text{Exp}(\\lambda)$ with density $f_\\lambda(x) = \\lambda e^{-\\lambda x}$ for $x > 0$. Consider testing $H_0: \\lambda = \\lambda_0$ versus $H_1: \\lambda = \\lambda_1$ where $\\lambda_1 > \\lambda_0$.\n",
    "\n",
    "Find the likelihood ratio $\\text{LR}(\\mathbf{X})$ and use part **(a)** to show that the NP test rejects for small values of $\\bar{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem2c",
   "metadata": {},
   "source": [
    "**(c)** Let $X_1, X_2, \\ldots, X_n$ be i.i.d. $N(\\mu, \\sigma^2)$ where $\\sigma^2$ is known, with density $f_\\mu(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\!\\left\\{-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right\\}$. Consider testing $H_0: \\mu = \\mu_0$ versus $H_1: \\mu = \\mu_1$ where $\\mu_1 > \\mu_0$.\n",
    "\n",
    "Find the likelihood ratio $\\text{LR}(\\mathbf{X})$ and use part **(a)** to show that the NP test rejects for large values of $\\bar{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem2d",
   "metadata": {},
   "source": [
    "**(d)** Let $X \\sim \\text{Binomial}(n, p)$ with PMF $f_p(x) = \\binom{n}{x} p^x (1-p)^{n-x}$ for $x \\in \\{0, 1, \\ldots, n\\}$. Consider testing $H_0: p = p_0$ versus $H_1: p = p_1$ where $p_1 > p_0$.\n",
    "\n",
    "Find the likelihood ratio $\\text{LR}(X)$ and use part **(a)** to show that the NP test rejects for large values of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem2e",
   "metadata": {},
   "source": [
    "Looking at parts **(b)**–**(d)**, you should notice that the NP test statistic is always a function of the sample mean (or of the single observation $X$ in the binomial case). Problem 4 will explain why this is not a coincidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem3-title",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## 4. Exponential Families\n",
    "\n",
    "A parametric family of distributions with densities (or PMFs) $f_\\theta$ is called a **one-parameter exponential family** if $f_\\theta$ can be written in the form\n",
    "\n",
    "$$f_\\theta(x) = h(x) \\exp\\!\\big\\{\\eta(\\theta)\\, T(x) - A(\\theta)\\big\\}$$\n",
    "\n",
    "where the different components have conventional names:\n",
    "- $T(x)$ is the **sufficient statistic** (a function of the data, not of $\\theta$),\n",
    "- $\\eta(\\theta)$ is the **natural parameter** (a function of $\\theta$ only),\n",
    "- $A(\\theta)$ is the **log-partition function** (ensures the density integrates to 1),\n",
    "- $h(x) \\geq 0$ is the **base density** (does not depend on $\\theta$).\n",
    "\n",
    "Note that this decomposition is not unique: for example, you could replace $T(x)$ by $2T(x)$ and $\\eta(\\theta)$ by $\\eta(\\theta)/2$ and still have a valid exponential family representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem3a",
   "metadata": {},
   "source": [
    "**(a)** Show that each of the three distributions from Problem 3 — $\\text{Exp}(\\lambda)$, $N(\\mu, \\sigma^2)$ with $\\sigma^2$ known, and $\\text{Binomial}(n, p)$ — is a one-parameter exponential family by identifying $T(x)$, $\\eta(\\theta)$, $A(\\theta)$, and $h(x)$ for each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem3b",
   "metadata": {},
   "source": [
    "**(b)** Now consider any one-parameter exponential family $f_\\theta(x) = h(x) \\exp\\!\\big\\{\\eta(\\theta)\\, T(x) - A(\\theta)\\big\\}$, and suppose we observe $X_1, X_2, \\ldots, X_n$ i.i.d. from $f_\\theta$.\n",
    "\n",
    "Consider the simple-vs-simple test $H_0: \\theta = \\theta_0$ versus $H_1: \\theta = \\theta_1$ where $\\eta(\\theta_1) > \\eta(\\theta_0)$.\n",
    "\n",
    "Show that the likelihood ratio $\\text{LR}(\\mathbf{X})$ is a strictly increasing function of $\\sum_{i=1}^n T(X_i)$, and conclude that the NP most powerful test rejects for large values of $\\frac{1}{n}\\sum_{i=1}^n T(X_i)$ (or for small values when $\\eta(\\theta_1) < \\eta(\\theta_0)$).\n",
    "\n",
    "Check that this is consistent with your answers in Problem 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem4-title",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## 5. The Cauchy Likelihood Ratio Test\n",
    "\n",
    "The **Cauchy location model** has density\n",
    "\n",
    "$$f_\\theta(x) = \\frac{1}{\\pi(1 + (x - \\theta)^2)}, \\qquad x \\in \\mathbb{R}.$$\n",
    "\n",
    "This is a location family (like the Normal), but with much heavier tails: the Cauchy distribution has no finite mean or variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem4a",
   "metadata": {},
   "source": [
    "**(a)** Let $X_1, X_2, \\ldots, X_n$ be i.i.d. $\\text{Cauchy}(\\theta)$. Consider testing $H_0: \\theta = \\theta_0$ versus $H_1: \\theta = \\theta_1$ where $\\theta_1 > \\theta_0$.\n",
    "\n",
    "Write down the log-likelihood ratio $\\log \\text{LR}(\\mathbf{X}) = \\sum_{i=1}^n \\log\\!\\left(\\frac{f_{\\theta_1}(X_i)}{f_{\\theta_0}(X_i)}\\right)$ explicitly.\n",
    "\n",
    "For a single observation $X$, plot the per-observation log-likelihood ratio $g(x) = \\log\\!\\left(\\frac{f_{\\theta_1}(x)}{f_{\\theta_0}(x)}\\right)$ over $x \\in [-10, 10]$ for two cases on the same axes: $(\\theta_0, \\theta_1) = (0, 1)$ and $(\\theta_0, \\theta_1) = (0, 5)$. Describe the shapes: where does each curve peak? Are they monotone in $x$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "problem4a-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem4b",
   "metadata": {},
   "source": [
    "**(b)** In Problem 3(b), you showed that for exponential families, $\\log \\text{LR}$ is a **linear** function of $\\sum T(X_i)$ — so the NP test rejects for large (or small) values of $\\bar{X}$ regardless of which specific $\\theta_1$ is the alternative.\n",
    "\n",
    "The Cauchy distribution is **not** an exponential family. Looking at your plots from part **(a)**, explain why no single monotone function of $\\bar{X}$ can be the most powerful test against all alternatives $\\theta_1 > 0$ simultaneously. What changes about the optimal NP test statistic as $\\theta_1$ changes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem4c",
   "metadata": {},
   "source": [
    "**(c)** Now specialize to testing $H_0: \\theta = 0$ versus $H_1: \\theta = 1$. Compare the NP test with the naive test that rejects for large $\\bar{X}$, across three sample sizes: $n = 1$, $n = 20$, and $n = 100$.\n",
    "\n",
    "For each value of $n$, simulate $N = 10{,}000$ samples from the Cauchy distribution under both the null ($\\theta = 0$) and alternative ($\\theta = 1$).\n",
    "\n",
    "For the naive test:\n",
    "- Under $H_0$, compute $\\bar{X}$ for each sample and find the 95th percentile as the cutoff $c$ (so the test has level $\\alpha = 0.05$).\n",
    "- Under $H_1$, compute the power: the proportion of samples where $\\bar{X} > c$.\n",
    "\n",
    "For the NP test:\n",
    "- Under $H_0$, compute $\\log \\text{LR}(\\mathbf{X})$ for each sample and find the 95th percentile as the cutoff.\n",
    "- Under $H_1$, compute the power.\n",
    "\n",
    "Report the power of both tests for each $n$ in a table. What pattern do you notice for each test as $n$ grows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "problem4c-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yfhh01nnzgm",
   "metadata": {},
   "source": [
    "**(d)** It is a fact (which you do not need to prove) that if $X_1, X_2, \\ldots, X_n$ are i.i.d. $\\text{Cauchy}(\\theta)$, then $\\bar{X}$ also has the $\\text{Cauchy}(\\theta)$ distribution, regardless of $n$. Use this to explain why the naive test's power does not improve as $n$ grows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
