{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type># USGS Earthquakes \u2192 Gardner\u2013Knopoff Declustering \u2192 California Catalog\n\nThis notebook:\n1. Downloads earthquakes from the USGS ComCat (FDSN event web service) for a *buffered* bounding box.\n2. Applies Gardner\u2013Knopoff (1974) window declustering to label dependent events (foreshocks/aftershocks).\n3. Clips to **California** using a state boundary polygon.\n4. Saves all earthquakes with `is_mainshock` flag to CSV.\n5. Computes and visualizes interarrival times (mainshocks only) for a Poisson-process sanity check.\n\nKey references:\n- USGS FDSN Event Web Service parameters (bbox, time, magnitude, etc.).\n- van Stiphout et al. (2012) CORSSA review summarizing Gardner\u2013Knopoff windows (Table 2) and discussing **censoring** and why using a buffered region matters."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "If you don't have the geospatial stack installed, uncomment and run the `pip install` cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if needed\n",
    "# !pip -q install pandas numpy requests geopandas shapely pyproj pyogrio matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 160)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Choose your study window + buffered download region\n",
    "\n",
    "Why buffered? Because declustering depends on events just outside your final region of interest.\n",
    "\n",
    "For a class demo, a simple buffer that covers CA + NV + neighboring borders is usually fine:\n",
    "- lon in [-126, -112]\n",
    "- lat in [31, 43.5]\n",
    "\n",
    "Then later we clip to California only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis choices\n",
    "MIN_MAG = 4.0\n",
    "\n",
    "# Your *analysis* time window (what you'll report results for)\n",
    "ANALYSIS_START = \"1980-01-01\"\n",
    "ANALYSIS_END   = \"2025-12-31\"\n",
    "\n",
    "# Add time padding for declustering windows (recommended)\n",
    "PAD_DAYS_BEFORE = 365   # 1 year\n",
    "PAD_DAYS_AFTER  = 365   # 1 year\n",
    "\n",
    "# Buffered bounding box (CA + NV + neighbors)\n",
    "MIN_LON, MAX_LON = -126.0, -112.0\n",
    "MIN_LAT, MAX_LAT = 31.0, 43.5\n",
    "\n",
    "analysis_start_dt = pd.to_datetime(ANALYSIS_START)\n",
    "analysis_end_dt   = pd.to_datetime(ANALYSIS_END)\n",
    "\n",
    "download_start_dt = analysis_start_dt - pd.Timedelta(days=PAD_DAYS_BEFORE)\n",
    "download_end_dt   = analysis_end_dt + pd.Timedelta(days=PAD_DAYS_AFTER)\n",
    "\n",
    "DOWNLOAD_START = download_start_dt.strftime('%Y-%m-%d')\n",
    "DOWNLOAD_END   = download_end_dt.strftime('%Y-%m-%d')\n",
    "\n",
    "DOWNLOAD_START, DOWNLOAD_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Download from the USGS event API (GeoJSON)\n",
    "\n",
    "USGS endpoint (FDSN Event Web Service):\n",
    "`https://earthquake.usgs.gov/fdsnws/event/1/query`\n",
    "\n",
    "We'll page using `limit` + `offset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USGS_ENDPOINT = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "\n",
    "def fetch_usgs_geojson(\n",
    "    starttime: str,\n",
    "    endtime: str,\n",
    "    minmagnitude: float,\n",
    "    minlatitude: float,\n",
    "    maxlatitude: float,\n",
    "    minlongitude: float,\n",
    "    maxlongitude: float,\n",
    "    limit: int = 20000,\n",
    "    polite_sleep_s: float = 0.5,\n",
    "    max_pages: int = 1000,\n",
    "):\n",
    "    \"\"\"Fetch events from USGS FDSN Event service using pagination.\n",
    "\n",
    "    Returns a list of GeoJSON features.\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    offset = 1  # USGS uses 1-based offsets\n",
    "\n",
    "    for page in range(max_pages):\n",
    "        params = {\n",
    "            \"format\": \"geojson\",\n",
    "            \"starttime\": starttime,\n",
    "            \"endtime\": endtime,\n",
    "            \"minmagnitude\": minmagnitude,\n",
    "            \"minlatitude\": minlatitude,\n",
    "            \"maxlatitude\": maxlatitude,\n",
    "            \"minlongitude\": minlongitude,\n",
    "            \"maxlongitude\": maxlongitude,\n",
    "            \"orderby\": \"time-asc\",\n",
    "            \"limit\": limit,\n",
    "            \"offset\": offset,\n",
    "        }\n",
    "\n",
    "        r = requests.get(USGS_ENDPOINT, params=params, timeout=60)\n",
    "        if r.status_code == 429:\n",
    "            # rate-limited\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        feats = data.get(\"features\", [])\n",
    "        if not feats:\n",
    "            break\n",
    "\n",
    "        all_features.extend(feats)\n",
    "\n",
    "        # If we got fewer than 'limit', we're done\n",
    "        if len(feats) < limit:\n",
    "            break\n",
    "\n",
    "        offset += len(feats)\n",
    "        time.sleep(polite_sleep_s)\n",
    "\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def features_to_dataframe(features):\n",
    "    rows = []\n",
    "    for f in features:\n",
    "        prop = f.get(\"properties\", {})\n",
    "        geom = f.get(\"geometry\", {})\n",
    "        coords = geom.get(\"coordinates\", [None, None, None])\n",
    "        lon, lat, depth_km = coords[0], coords[1], coords[2] if len(coords) > 2 else None\n",
    "        t_ms = prop.get(\"time\")\n",
    "        t = pd.to_datetime(t_ms, unit=\"ms\", utc=True) if t_ms is not None else pd.NaT\n",
    "        rows.append({\n",
    "            \"id\": f.get(\"id\"),\n",
    "            \"time\": t,\n",
    "            \"mag\": prop.get(\"mag\"),\n",
    "            \"place\": prop.get(\"place\"),\n",
    "            \"type\": prop.get(\"type\"),\n",
    "            \"lon\": lon,\n",
    "            \"lat\": lat,\n",
    "            \"depth_km\": depth_km,\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.dropna(subset=[\"time\", \"mag\", \"lon\", \"lat\"]).copy()\n",
    "    df[\"mag\"] = df[\"mag\"].astype(float)\n",
    "    df = df.sort_values(\"time\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "features = fetch_usgs_geojson(\n",
    "    starttime=DOWNLOAD_START,\n",
    "    endtime=DOWNLOAD_END,\n",
    "    minmagnitude=MIN_MAG,\n",
    "    minlatitude=MIN_LAT,\n",
    "    maxlatitude=MAX_LAT,\n",
    "    minlongitude=MIN_LON,\n",
    "    maxlongitude=MAX_LON,\n",
    ")\n",
    "\n",
    "df = features_to_dataframe(features)\n",
    "df.head(), len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Gardner\u2013Knopoff (1974) windows + declustering\n",
    "\n",
    "We use the original Gardner & Knopoff (1974) **magnitude-dependent space/time window formulas**:\n",
    "\n",
    "- **Distance window**: $L = 10^{0.1238 M + 0.983}$ km\n",
    "- **Time window**: $T = 10^{0.5409 M - 0.547}$ days for $M < 6.5$, or $T = 10^{0.032 M + 2.7389}$ days for $M \\geq 6.5$\n",
    "\n",
    "Implementation approach (similar spirit to OpenQuake's GK declusterer):\n",
    "- Sort events by **descending magnitude**.\n",
    "- For each not-yet-assigned event, declare it a potential mainshock.\n",
    "- Mark any not-yet-assigned events within the GK distance window and within a **time window before/after** as dependent.\n",
    "\n",
    "We use a symmetric window (foreshocks + aftershocks) by setting `FS_TIME_PROP = 1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gardner-Knopoff (1974) window formulas\n",
    "def gk_distance_km(M):\n",
    "    \"\"\"GK1974 distance window: L = 10^(0.1238*M + 0.983) km\"\"\"\n",
    "    return 10 ** (0.1238 * M + 0.983)\n",
    "\n",
    "def gk_time_days(M):\n",
    "    \"\"\"GK1974 time window:\n",
    "    T = 10^(0.5409*M - 0.547) days for M < 6.5\n",
    "    T = 10^(0.032*M + 2.7389) days for M >= 6.5\n",
    "    \"\"\"\n",
    "    M = np.asarray(M)\n",
    "    T = np.where(M >= 6.5,\n",
    "                 10 ** (0.032 * M + 2.7389),\n",
    "                 10 ** (0.5409 * M - 0.547))\n",
    "    return T\n",
    "\n",
    "def gk_windows(mags: np.ndarray):\n",
    "    \"\"\"Compute GK distance and time windows for given magnitudes.\"\"\"\n",
    "    return gk_distance_km(mags), gk_time_days(mags)\n",
    "\n",
    "\n",
    "def haversine_km(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"Vectorized haversine distance in km.\"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    R = 6371.0088\n",
    "    return R * c\n",
    "\n",
    "\n",
    "def gardner_knopoff_decluster(df_events: pd.DataFrame, fs_time_prop: float = 1.0):\n",
    "    \"\"\"Decluster using a GK-style mainshock window method.\n",
    "\n",
    "    Returns df with extra columns:\n",
    "      - cluster_id (0 means unclustered/singleton)\n",
    "      - is_dependent (True if in a cluster but not the main event)\n",
    "      - is_mainshock (True if representative of its cluster or singleton)\n",
    "    \"\"\"\n",
    "    df0 = df_events.sort_values(\"time\").reset_index(drop=True).copy()\n",
    "    n = len(df0)\n",
    "\n",
    "    # Precompute windows per event magnitude\n",
    "    L_km, T_days = gk_windows(df0[\"mag\"].values)\n",
    "    df0[\"L_km\"] = L_km\n",
    "    df0[\"T_days\"] = T_days\n",
    "\n",
    "    # For speed\n",
    "    times = df0[\"time\"].values.astype(\"datetime64[ns]\")\n",
    "    t_days = times.astype(\"datetime64[ns]\").astype(\"int64\") / (1e9 * 86400.0)\n",
    "    lon = df0[\"lon\"].values\n",
    "    lat = df0[\"lat\"].values\n",
    "    mag = df0[\"mag\"].values\n",
    "\n",
    "    # Work in descending magnitude like OpenQuake (largest event anchors a cluster)\n",
    "    idx_desc = np.argsort(-mag, kind=\"mergesort\")\n",
    "\n",
    "    cluster_id = np.zeros(n, dtype=int)\n",
    "    is_dependent = np.zeros(n, dtype=bool)\n",
    "\n",
    "    next_cluster = 1\n",
    "    for ii in idx_desc:\n",
    "        if cluster_id[ii] != 0:\n",
    "            continue\n",
    "\n",
    "        # Candidate mainshock ii: find unassigned events within its time window\n",
    "        dt = t_days - t_days[ii]\n",
    "        in_time = (cluster_id == 0) & (dt >= -T_days[ii] * fs_time_prop) & (dt <= T_days[ii])\n",
    "        if not np.any(in_time):\n",
    "            continue\n",
    "\n",
    "        # Now apply distance window among those in time window\n",
    "        cand = np.where(in_time)[0]\n",
    "        dkm = haversine_km(lon[cand], lat[cand], lon[ii], lat[ii])\n",
    "        in_both = cand[dkm <= L_km[ii]]\n",
    "\n",
    "        # Exclude itself; if nothing else, leave as singleton (cluster_id stays 0)\n",
    "        in_both_wo_self = in_both[in_both != ii]\n",
    "        if in_both_wo_self.size == 0:\n",
    "            continue\n",
    "\n",
    "        # Assign cluster id to all including mainshock\n",
    "        cluster_id[in_both] = next_cluster\n",
    "        # Dependent: everything except mainshock index ii\n",
    "        is_dependent[in_both_wo_self] = True\n",
    "        is_dependent[ii] = False\n",
    "        next_cluster += 1\n",
    "\n",
    "    df0[\"cluster_id\"] = cluster_id\n",
    "    df0[\"is_dependent\"] = is_dependent\n",
    "    df0[\"is_mainshock\"] = ~df0[\"is_dependent\"]\n",
    "\n",
    "    return df0\n",
    "\n",
    "\n",
    "df_gk = gardner_knopoff_decluster(df, fs_time_prop=1.0)\n",
    "df_gk[[\"time\",\"mag\",\"lat\",\"lon\",\"cluster_id\",\"is_dependent\",\"is_mainshock\"]].head(10), df_gk[\"is_mainshock\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Clip to California (state boundary polygon)\n",
    "\n",
    "We use the U.S. Census Bureau TIGER/Line state boundary shapefile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# TIGER/Line states shapefile (2024) zip\n",
    "TIGER_URL = \"https://www2.census.gov/geo/tiger/TIGER2024/STATE/tl_2024_us_state.zip\"\n",
    "ZIP_PATH = DATA_DIR / \"tl_2024_us_state.zip\"\n",
    "\n",
    "if not ZIP_PATH.exists():\n",
    "    print(\"Downloading TIGER state boundaries\u2026\")\n",
    "    r = requests.get(TIGER_URL, stream=True, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    with open(ZIP_PATH, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "# Read CA polygon directly from the zip\n",
    "states = gpd.read_file(f\"zip://{ZIP_PATH}\")\n",
    "ca = states[states[\"NAME\"] == \"California\"].to_crs(\"EPSG:4326\")\n",
    "ca = ca[[\"NAME\",\"geometry\"]].reset_index(drop=True)\n",
    "ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert events to GeoDataFrame\ngdf = gpd.GeoDataFrame(\n    df_gk,\n    geometry=gpd.points_from_xy(df_gk[\"lon\"], df_gk[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n# Clip ALL earthquakes to CA (keep is_mainshock flag for later filtering)\ngdf_ca = gdf[gdf.within(ca.geometry.iloc[0])].copy()\n\n# Restrict to your *analysis* time window\ngdf_ca = gdf_ca[(gdf_ca[\"time\"] >= pd.to_datetime(ANALYSIS_START, utc=True)) &\n                (gdf_ca[\"time\"] <= pd.to_datetime(ANALYSIS_END, utc=True))].copy()\n\nprint(\"Total downloaded (buffered bbox):\", len(df_gk))\nprint(\"CA earthquakes (clipped):\", len(gdf_ca))\nprint(\"  - Mainshocks:\", gdf_ca[\"is_mainshock\"].sum())\nprint(\"  - Dependent (fore/aftershocks):\", (~gdf_ca[\"is_mainshock\"]).sum())\n\ngdf_ca[[\"time\",\"mag\",\"place\",\"is_mainshock\",\"cluster_id\"]].head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Interarrival times + quick Poisson sanity checks\n",
    "\n",
    "For a homogeneous Poisson process, interarrival times are i.i.d. exponential.\n",
    "Below: histogram + empirical CDF vs fitted exponential, plus an exponential Q-Q plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export dataframe (all earthquakes, with is_mainshock flag)\ndf_ca = pd.DataFrame(gdf_ca.drop(columns=\"geometry\"))\ndf_ca = df_ca.sort_values(\"time\").reset_index(drop=True)\n\n# For interarrival analysis, filter to mainshocks only\ndf_mainshocks = df_ca[df_ca[\"is_mainshock\"]].copy().reset_index(drop=True)\n\n# Interarrival times in days (mainshocks only)\nt = df_mainshocks[\"time\"].values.astype(\"datetime64[ns]\")\ndt_days = (t[1:] - t[:-1]).astype(\"timedelta64[s]\").astype(float) / 86400.0\ndt_days = pd.Series(dt_days, name=\"interarrival_days\")\n\nprint(f\"Mainshocks: {len(df_mainshocks)}\")\nprint(f\"Interarrival times: {len(dt_days)}\")\ndt_days.describe()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit exponential by MLE: lambda = 1/mean\n",
    "mean_dt = dt_days.mean()\n",
    "lam = 1.0 / mean_dt\n",
    "\n",
    "x = np.linspace(0, np.quantile(dt_days, 0.99), 300)\n",
    "F_exp = 1 - np.exp(-lam * x)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(dt_days, bins=40, density=True)\n",
    "plt.plot(x, lam * np.exp(-lam * x), linewidth=2)\n",
    "plt.xlabel(\"Interarrival time (days)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Interarrival times: histogram + fitted exponential\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "sorted_dt = np.sort(dt_days.values)\n",
    "ecdf = np.arange(1, len(sorted_dt)+1) / len(sorted_dt)\n",
    "plt.plot(sorted_dt, ecdf, label=\"Empirical CDF\")\n",
    "plt.plot(x, F_exp, linewidth=2, label=\"Fitted Exp CDF\")\n",
    "plt.xlabel(\"Interarrival time (days)\")\n",
    "plt.ylabel(\"CDF\")\n",
    "plt.title(\"ECDF vs fitted exponential CDF\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential Q-Q plot: compare sorted_dt to theoretical quantiles\n",
    "p = (np.arange(1, len(sorted_dt)+1) - 0.5) / len(sorted_dt)\n",
    "q_theory = -np.log(1 - p) / lam\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(q_theory, sorted_dt, s=10)\n",
    "mx = max(q_theory.max(), sorted_dt.max())\n",
    "plt.plot([0, mx], [0, mx], linewidth=2)\n",
    "plt.xlabel(\"Theoretical Exp quantiles (days)\")\n",
    "plt.ylabel(\"Empirical quantiles (days)\")\n",
    "plt.title(\"Exponential Q-Q plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Export (optional)\n",
    "\n",
    "Save your final declustered, California-only catalog to CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save all CA earthquakes (with is_mainshock flag) to CSV\nOUT_PATH = DATA_DIR / \"california_earthquakes_declustered.csv\"\ndf_ca.to_csv(OUT_PATH, index=False)\nprint(f\"Saved {len(df_ca)} earthquakes to {OUT_PATH}\")\nprint(f\"  - Mainshocks: {df_ca['is_mainshock'].sum()}\")\nprint(f\"  - Dependent: {(~df_ca['is_mainshock']).sum()}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}