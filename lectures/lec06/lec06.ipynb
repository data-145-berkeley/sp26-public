{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Lecture 6: Decision Theory — Loss, Risk, Admissibility, and Optimality\n",
    "\n",
    "**Data 145, Spring 2026: Evidence and Uncertainty**  \n",
    "**Instructors:** Ani Adhikari, William Fithian\n",
    "\n",
    "---\n",
    "\n",
    "**Please run the setup cell below before reading.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "# Color scheme for this lecture (colorblind-safe palette)\n",
    "# Using IBM's colorblind-safe palette\n",
    "COLOR_MLE = '#648FFF'       # Blue - MLE (unbiased)\n",
    "COLOR_LAPLACE1 = '#785EF0'  # Purple - Laplace +1/+1\n",
    "COLOR_LAPLACE2 = '#DC267F'  # Magenta - Laplace +2/+2  \n",
    "COLOR_BAD = '#FE6100'       # Orange - Inadmissible\n",
    "COLOR_TRUE = '#000000'      # Black - True value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## Introduction: Beyond Asymptotic Efficiency\n",
    "\n",
    "In Lectures 3–5, we developed the theory of maximum likelihood estimation. We showed that the MLE is:\n",
    "- **Consistent**: $\\hat{\\theta}_n \\stackrel{P}{\\to} \\theta_0$\n",
    "- **Asymptotically normal**: $\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\stackrel{d}{\\to} N(0, 1/I(\\theta_0))$\n",
    "- **Efficient**: Among unbiased estimators, it achieves the Cramér-Rao lower bound\n",
    "\n",
    "This is a powerful result: if you want an unbiased estimator with small variance, the MLE is (asymptotically) the best you can do.\n",
    "\n",
    "**But who said we have to be unbiased?**\n",
    "\n",
    "Today we'll see that:\n",
    "1. Biased estimators can sometimes have *lower* mean squared error than the MLE\n",
    "2. There's no single \"best\" estimator — different estimators are optimal under different criteria\n",
    "3. This leads naturally to **Bayesian statistics** as one principled way to choose among estimators\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decision-theory",
   "metadata": {},
   "source": [
    "## 1. Decision Theory in Statistical Estimation\n",
    "\n",
    "### The Setup\n",
    "\n",
    "Recall our standard setup:\n",
    "- **Data**: $X$ (often $X_1, \\ldots, X_n$ i.i.d.)\n",
    "- **Model**: $f_\\theta$ indexed by parameter $\\theta \\in \\Theta$\n",
    "- **Estimand**: Some function $g(\\theta)$ we want to estimate (often just $\\theta$ itself)\n",
    "- **Estimator**: A function $T(X)$ that produces our estimate\n",
    "\n",
    "When estimating $\\theta$ itself, we typically write $\\hat{\\theta}(X)$ or just $\\hat{\\theta}$.\n",
    "\n",
    "### Loss Functions\n",
    "\n",
    "A **loss function** $L(\\theta, a)$ measures how bad it is to report the estimate $a$ when the true parameter is $\\theta$.\n",
    "\n",
    "**Squared error loss** is the most common choice:\n",
    "$$L(\\theta, a) = (\\theta - a)^2$$\n",
    "\n",
    "Other loss functions exist (absolute error, 0-1 loss, etc.), but we'll focus on squared error.\n",
    "\n",
    "### Risk Functions\n",
    "\n",
    "The loss for a single dataset doesn't tell us much — one estimator might beat another by luck. We want to average over the sampling distribution.\n",
    "\n",
    "The **risk function** is the expected loss:\n",
    "$$R(\\theta, T) = E_\\theta[L(\\theta, T(X))]$$\n",
    "\n",
    "For squared error loss, this is the **mean squared error (MSE)**:\n",
    "$$R(\\theta, T) = \\text{MSE}_\\theta(T) = E_\\theta[(T(X) - \\theta)^2]$$\n",
    "\n",
    "### The Fundamental Problem\n",
    "\n",
    "The risk function depends on $\\theta$, which we don't know! Different estimators may be better for different values of $\\theta$.\n",
    "\n",
    "This creates a fundamental tension: **how do we compare estimators when their relative performance depends on the unknown parameter?**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binomial-setup",
   "metadata": {},
   "source": [
    "## 2. The Binomial Example\n",
    "\n",
    "### Setup\n",
    "\n",
    "Let's work with a concrete example where we can visualize everything.\n",
    "\n",
    "**Example (Coin flipping):** Flip a coin $n$ times. Let $X$ = number of heads. We want to estimate the probability $p$ of heads.\n",
    "\n",
    "Equivalently: We have $n$ patients in a drug trial, $X$ respond positively, and we want to estimate the success rate $p$.\n",
    "\n",
    "Mathematically: $X \\sim \\text{Binomial}(n, p)$, and we want to estimate $p \\in [0, 1]$.\n",
    "\n",
    "This example is nice because:\n",
    "- The parameter space $[0, 1]$ is bounded, so we can plot risk functions over the entire space\n",
    "- $X$ is the sum of $n$ i.i.d. Bernoulli($p$) random variables, so results from Lectures 4–5 apply\n",
    "\n",
    "We'll use $n = 16$ throughout (you'll see why this choice is nice later).\n",
    "\n",
    "### The MLE\n",
    "\n",
    "The MLE for $p$ is the sample proportion:\n",
    "$$\\hat{p} = \\frac{X}{n}$$\n",
    "\n",
    "From Lectures 4–5 (applied to Bernoulli observations), we know:\n",
    "- **Unbiased**: $E_p[\\hat{p}] = p$\n",
    "- **Variance**: $\\text{Var}_p(\\hat{p}) = \\frac{p(1-p)}{n}$\n",
    "- **Fisher information** (per observation): $I(p) = \\frac{1}{p(1-p)}$\n",
    "- **Efficient**: $\\text{Var}_p(\\hat{p}) = \\frac{1}{nI(p)}$ achieves the Cramér-Rao bound\n",
    "\n",
    "Since the MLE is unbiased, its MSE equals its variance:\n",
    "$$\\text{MSE}_p(\\hat{p}) = \\frac{p(1-p)}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mle-mse-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the MSE of the MLE\n",
    "n = 16\n",
    "p_grid = np.linspace(0.001, 0.999, 500)\n",
    "\n",
    "# MSE of MLE\n",
    "mse_mle = p_grid * (1 - p_grid) / n\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(p_grid, mse_mle, color=COLOR_MLE, linewidth=2.5, label=r'MLE: $\\hat{p} = X/n$')\n",
    "\n",
    "ax.set_xlabel('True probability $p$', fontsize=12)\n",
    "ax.set_ylabel('Mean Squared Error', fontsize=12)\n",
    "ax.set_title(f'MSE of the MLE (n = {n})', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, None)\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Maximum MSE occurs at p = 0.5: MSE = {0.5 * 0.5 / n:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mle-question",
   "metadata": {},
   "source": [
    "### Is Unbiased + Efficient the End of the Story?\n",
    "\n",
    "The MLE is unbiased and efficient. Is it the \"best\" estimator?\n",
    "\n",
    "Consider a concrete scenario: $n = 16$ flips, and we observe $X = 12$ heads.\n",
    "\n",
    "The MLE says $\\hat{p} = 12/16 = 0.75$.\n",
    "\n",
    "**Question:** Do we really believe the coin has a 75% chance of heads? Or might that be an overreaction to a small sample?\n",
    "\n",
    "Most real coins are close to fair. If we had some prior belief that $p$ is likely near 0.5, we might want to **shrink** our estimate toward 0.5.\n",
    "\n",
    "The MLE is the best *unbiased* estimator. **But who said we have to be unbiased?**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shrinkage",
   "metadata": {},
   "source": [
    "## 3. Shrinkage Estimators\n",
    "\n",
    "### Laplace's Estimator: Adding Pseudodata\n",
    "\n",
    "**Intuition:** If we think extreme values of $p$ (near 0 or 1) are unlikely, maybe we should shrink our estimate toward 1/2.\n",
    "\n",
    "**Laplace's idea (1774):** Pretend we observed two extra flips before seeing the data — one head and one tail. This gives:\n",
    "\n",
    "$$\\tilde{p}_1 = \\frac{X + 1}{n + 2}$$\n",
    "\n",
    "For our example with $n = 16$, $X = 12$:\n",
    "- MLE: $\\hat{p} = 12/16 = 0.75$\n",
    "- Laplace: $\\tilde{p}_1 = 13/18 \\approx 0.722$\n",
    "\n",
    "The Laplace estimator shrinks toward 0.5.\n",
    "\n",
    "### More Aggressive Shrinkage\n",
    "\n",
    "What if we add *two* heads and *two* tails of pseudodata?\n",
    "\n",
    "$$\\tilde{p}_2 = \\frac{X + 2}{n + 4}$$\n",
    "\n",
    "For $n = 16$, $X = 12$:\n",
    "- Laplace +2/+2: $\\tilde{p}_2 = 14/20 = 0.70$\n",
    "\n",
    "This shrinks even more toward 0.5.\n",
    "\n",
    "### An Inadmissible Estimator\n",
    "\n",
    "What about this estimator?\n",
    "\n",
    "$$\\hat{p}_{\\text{bad}} = \\frac{X + 1}{n}$$\n",
    "\n",
    "This adds 1 to the numerator but nothing to the denominator.\n",
    "\n",
    "**Note:** This does NOT correspond to adding pseudodata (that would be $(X+1)/(n+1)$). It's just inflating the estimate.\n",
    "\n",
    "For $n = 16$, $X = 12$:\n",
    "- Bad estimator: $\\hat{p}_{\\text{bad}} = 13/16 = 0.8125$ — even higher than the MLE!\n",
    "\n",
    "Let's see how all four estimators compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "all-estimators-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MSE functions for all four estimators\n",
    "def mse_mle(p, n):\n",
    "    \"\"\"MSE of MLE: X/n\"\"\"\n",
    "    return p * (1 - p) / n\n",
    "\n",
    "def mse_laplace1(p, n):\n",
    "    \"\"\"MSE of Laplace +1/+1: (X+1)/(n+2)\"\"\"\n",
    "    # E[(X+1)/(n+2)] = (np + 1)/(n+2)\n",
    "    # Bias = (np + 1)/(n+2) - p = (1 - 2p)/(n+2)\n",
    "    # Var = n * p(1-p) / (n+2)^2\n",
    "    bias = (1 - 2*p) / (n + 2)\n",
    "    var = n * p * (1 - p) / (n + 2)**2\n",
    "    return bias**2 + var\n",
    "\n",
    "def mse_laplace2(p, n):\n",
    "    \"\"\"MSE of Laplace +2/+2: (X+2)/(n+4)\"\"\"\n",
    "    # E[(X+2)/(n+4)] = (np + 2)/(n+4)\n",
    "    # Bias = (np + 2)/(n+4) - p = (2 - 4p)/(n+4) = 2(1 - 2p)/(n+4)\n",
    "    # Var = n * p(1-p) / (n+4)^2\n",
    "    bias = 2 * (1 - 2*p) / (n + 4)\n",
    "    var = n * p * (1 - p) / (n + 4)**2\n",
    "    return bias**2 + var\n",
    "\n",
    "def mse_bad(p, n):\n",
    "    \"\"\"MSE of bad estimator: (X+1)/n\"\"\"\n",
    "    # E[(X+1)/n] = p + 1/n\n",
    "    # Bias = 1/n (constant!)\n",
    "    # Var = p(1-p)/n (same as MLE)\n",
    "    bias = 1 / n\n",
    "    var = p * (1 - p) / n\n",
    "    return bias**2 + var\n",
    "\n",
    "# Compute MSE for all estimators\n",
    "n = 16\n",
    "p_grid = np.linspace(0.001, 0.999, 500)\n",
    "\n",
    "mse_vals = {\n",
    "    'MLE': mse_mle(p_grid, n),\n",
    "    'Laplace +1/+1': mse_laplace1(p_grid, n),\n",
    "    'Laplace +2/+2': mse_laplace2(p_grid, n),\n",
    "    'Bad': mse_bad(p_grid, n)\n",
    "}\n",
    "\n",
    "# Plot all MSE curves\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "ax.plot(p_grid, mse_vals['MLE'], color=COLOR_MLE, linewidth=2.5, \n",
    "        label=r'MLE: $\\hat{p} = X/n$')\n",
    "ax.plot(p_grid, mse_vals['Laplace +1/+1'], color=COLOR_LAPLACE1, linewidth=2.5,\n",
    "        label=r'Laplace +1/+1: $\\tilde{p}_1 = (X+1)/(n+2)$')\n",
    "ax.plot(p_grid, mse_vals['Laplace +2/+2'], color=COLOR_LAPLACE2, linewidth=2.5,\n",
    "        label=r'Laplace +2/+2: $\\tilde{p}_2 = (X+2)/(n+4)$')\n",
    "ax.plot(p_grid, mse_vals['Bad'], color=COLOR_BAD, linewidth=2.5, linestyle='--',\n",
    "        label=r'Bad: $\\hat{p}_{bad} = (X+1)/n$')\n",
    "\n",
    "ax.set_xlabel('True probability $p$', fontsize=12)\n",
    "ax.set_ylabel('Mean Squared Error', fontsize=12)\n",
    "ax.set_title(f'MSE Comparison of Four Estimators (n = {n})', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 0.02)\n",
    "ax.legend(fontsize=11, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mse-observations",
   "metadata": {},
   "source": [
    "### Observations from the Plot\n",
    "\n",
    "1. **The bad estimator is strictly worse than the MLE everywhere!** Its MSE curve lies entirely above the MLE curve.\n",
    "\n",
    "2. **The other curves cross.** No single estimator (among MLE, Laplace +1/+1, Laplace +2/+2) is best for all values of $p$.\n",
    "\n",
    "3. **Near $p = 0.5$**, the Laplace estimators beat the MLE. Their shrinkage toward 0.5 helps when $p$ really is near 0.5.\n",
    "\n",
    "4. **Near $p = 0$ or $p = 1$**, the MLE beats the Laplace estimators. Shrinking toward 0.5 hurts when the truth is extreme.\n",
    "\n",
    "To understand *why* these patterns occur, we need to examine the bias-variance tradeoff.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bias-variance",
   "metadata": {},
   "source": [
    "## 4. Bias-Variance Calculations and Admissibility\n",
    "\n",
    "### The Bias-Variance Decomposition\n",
    "\n",
    "Recall that for any estimator $T$:\n",
    "$$\\text{MSE}_\\theta(T) = \\text{Bias}_\\theta(T)^2 + \\text{Var}_\\theta(T)$$\n",
    "\n",
    "where $\\text{Bias}_\\theta(T) = E_\\theta[T] - \\theta$.\n",
    "\n",
    "Let's calculate the bias, variance, and MSE for each of our four estimators.\n",
    "\n",
    "### MLE: $\\hat{p} = X/n$\n",
    "\n",
    "**Mean:** $E_p[\\hat{p}] = E_p[X/n] = p$\n",
    "\n",
    "**Bias:** $\\text{Bias}_p(\\hat{p}) = p - p = 0$\n",
    "\n",
    "**Variance:** $\\text{Var}_p(\\hat{p}) = \\frac{\\text{Var}_p(X)}{n^2} = \\frac{np(1-p)}{n^2} = \\frac{p(1-p)}{n}$\n",
    "\n",
    "**MSE:** $\\text{MSE}_p(\\hat{p}) = 0 + \\frac{p(1-p)}{n} = \\frac{p(1-p)}{n}$\n",
    "\n",
    "### Laplace +1/+1: $\\tilde{p}_1 = (X+1)/(n+2)$\n",
    "\n",
    "**Mean:** $E_p[\\tilde{p}_1] = \\frac{E_p[X] + 1}{n+2} = \\frac{np + 1}{n+2}$\n",
    "\n",
    "**Bias:** \n",
    "$$\\text{Bias}_p(\\tilde{p}_1) = \\frac{np + 1}{n+2} - p = \\frac{np + 1 - p(n+2)}{n+2} = \\frac{np + 1 - np - 2p}{n+2} = \\frac{1 - 2p}{n+2}$$\n",
    "\n",
    "Notice: the bias is **positive when $p < 0.5$** (pulls estimate up toward 0.5) and **negative when $p > 0.5$** (pulls estimate down toward 0.5). The estimator shrinks toward 0.5!\n",
    "\n",
    "**Variance:** $\\text{Var}_p(\\tilde{p}_1) = \\frac{\\text{Var}_p(X)}{(n+2)^2} = \\frac{np(1-p)}{(n+2)^2}$\n",
    "\n",
    "Note: the variance is **smaller** than the MLE's variance $p(1-p)/n$ because $(n+2)^2 > n \\cdot n$ and the numerator only has $n$ (not $n+2$).\n",
    "\n",
    "**MSE:** \n",
    "$$\\text{MSE}_p(\\tilde{p}_1) = \\frac{(1-2p)^2}{(n+2)^2} + \\frac{np(1-p)}{(n+2)^2} = \\frac{(1-2p)^2 + np(1-p)}{(n+2)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bias-variance-2",
   "metadata": {},
   "source": [
    "### Laplace +2/+2: $\\tilde{p}_2 = (X+2)/(n+4)$\n",
    "\n",
    "**Mean:** $E_p[\\tilde{p}_2] = \\frac{np + 2}{n+4}$\n",
    "\n",
    "**Bias:** \n",
    "$$\\text{Bias}_p(\\tilde{p}_2) = \\frac{np + 2}{n+4} - p = \\frac{np + 2 - p(n+4)}{n+4} = \\frac{2 - 4p}{n+4} = \\frac{2(1 - 2p)}{n+4}$$\n",
    "\n",
    "Same pattern as before, but with more shrinkage toward 0.5.\n",
    "\n",
    "**Variance:** $\\text{Var}_p(\\tilde{p}_2) = \\frac{np(1-p)}{(n+4)^2}$\n",
    "\n",
    "Even smaller variance than Laplace +1/+1.\n",
    "\n",
    "**MSE:** \n",
    "$$\\text{MSE}_p(\\tilde{p}_2) = \\frac{4(1-2p)^2}{(n+4)^2} + \\frac{np(1-p)}{(n+4)^2} = \\frac{4(1-2p)^2 + np(1-p)}{(n+4)^2}$$\n",
    "\n",
    "### Bad Estimator: $\\hat{p}_{\\text{bad}} = (X+1)/n$\n",
    "\n",
    "**Mean:** $E_p[\\hat{p}_{\\text{bad}}] = \\frac{E_p[X] + 1}{n} = \\frac{np + 1}{n} = p + \\frac{1}{n}$\n",
    "\n",
    "**Bias:** $\\text{Bias}_p(\\hat{p}_{\\text{bad}}) = \\frac{1}{n}$ — **positive for all $p$!**\n",
    "\n",
    "**Variance:** $\\text{Var}_p(\\hat{p}_{\\text{bad}}) = \\frac{\\text{Var}_p(X)}{n^2} = \\frac{p(1-p)}{n}$ — **same as the MLE!**\n",
    "\n",
    "**MSE:** \n",
    "$$\\text{MSE}_p(\\hat{p}_{\\text{bad}}) = \\frac{1}{n^2} + \\frac{p(1-p)}{n} = \\text{MSE}_p(\\hat{p}) + \\frac{1}{n^2}$$\n",
    "\n",
    "The bad estimator has the same variance as the MLE but strictly positive bias. Its MSE is strictly higher than the MLE's MSE for all $p$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bias-variance-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of bias, variance, MSE formulas\n",
    "print(\"Summary of Estimator Properties\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Estimator':<25} {'Bias':^20} {'Variance':^20} {'MSE':^20}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'MLE: X/n':<25} {'0':^20} {'p(1-p)/n':^20} {'p(1-p)/n':^20}\")\n",
    "print(f\"{'Laplace +1: (X+1)/(n+2)':<25} {'(1-2p)/(n+2)':^20} {'np(1-p)/(n+2)²':^20} {'[formula]':^20}\")\n",
    "print(f\"{'Laplace +2: (X+2)/(n+4)':<25} {'2(1-2p)/(n+4)':^20} {'np(1-p)/(n+4)²':^20} {'[formula]':^20}\")\n",
    "print(f\"{'Bad: (X+1)/n':<25} {'1/n':^20} {'p(1-p)/n':^20} {'p(1-p)/n + 1/n²':^20}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-went-wrong",
   "metadata": {},
   "source": [
    "### What Went Wrong with the Bad Estimator?\n",
    "\n",
    "The bad estimator has:\n",
    "- **Same variance** as the MLE\n",
    "- **Strictly positive bias** for all $p$\n",
    "\n",
    "It adds bias without any compensating variance reduction. That's a bad trade!\n",
    "\n",
    "The Laplace estimators, by contrast, make a **smart trade**:\n",
    "- They add bias (toward 0.5)\n",
    "- But they reduce variance (by inflating the denominator)\n",
    "- When $p$ is near 0.5, the bias is small and the variance reduction helps\n",
    "- When $p$ is near 0 or 1, the bias penalty outweighs the variance benefit\n",
    "\n",
    "The bad estimator just shifts everything up — pure bias, no variance benefit.\n",
    "\n",
    "### Admissibility\n",
    "\n",
    "**Definition:** An estimator $T_1$ is **inadmissible** if there exists another estimator $T_2$ such that:\n",
    "- $R(\\theta, T_2) \\le R(\\theta, T_1)$ for all $\\theta \\in \\Theta$\n",
    "- $R(\\theta, T_2) < R(\\theta, T_1)$ for at least one $\\theta$\n",
    "\n",
    "In this case, we say $T_2$ **dominates** $T_1$. An estimator that is not inadmissible is called **admissible**.\n",
    "\n",
    "**Our estimators:**\n",
    "- The **bad estimator is inadmissible** — it is dominated by the MLE\n",
    "- The **MLE, Laplace +1/+1, and Laplace +2/+2 are all admissible** — none dominates another\n",
    "\n",
    "Admissibility is a minimal requirement: we should never use an inadmissible estimator. But among admissible estimators, we still need a way to choose.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimality",
   "metadata": {},
   "source": [
    "## 5. Optimality Criteria\n",
    "\n",
    "### The Fundamental Problem\n",
    "\n",
    "We have multiple admissible estimators. How do we choose among them?\n",
    "\n",
    "**Question:** Is it possible to find an estimator that's best for *all* values of $p$?\n",
    "\n",
    "**Answer:** No! (At least not for MSE.)\n",
    "\n",
    "**Why not?** Think about it: if $p = 0.75$ is really the truth, then the constant estimator $T(X) \\equiv 0.75$ has MSE = 0 at that point. No other estimator can beat it there. But that constant estimator would be terrible if $p = 0.25$.\n",
    "\n",
    "No estimator can beat every other estimator at every $p$.\n",
    "\n",
    "### Two Approaches to Choosing Among Admissible Estimators\n",
    "\n",
    "**Approach 1: Restrict to a class of estimators**\n",
    "\n",
    "For example, among **unbiased** estimators, the MLE is optimal.\n",
    "\n",
    "*Why?* For unbiased estimators, MSE = Variance. So minimizing MSE is the same as minimizing variance. By the Cramér-Rao bound, the MLE achieves the minimum variance among unbiased estimators.\n",
    "\n",
    "Another example: among **equivariant** estimators (you'll see this in Worksheet 3, Problem 5), there's often a unique best choice.\n",
    "\n",
    "**Note:** We won't pursue optimality among restricted classes further in this course. Instead, we'll focus on the second approach.\n",
    "\n",
    "**Approach 2: Summarize the risk function by a single number**\n",
    "\n",
    "Instead of comparing entire risk curves, we can reduce each estimator to a single number and compare those.\n",
    "\n",
    "### Worst-Case Risk (Minimax)\n",
    "\n",
    "Define the **worst-case risk**:\n",
    "$$R_{\\max}(T) = \\max_{p \\in [0,1]} \\text{MSE}_p(T)$$\n",
    "\n",
    "A **minimax estimator** minimizes the worst-case risk.\n",
    "\n",
    "This is a conservative approach: we guard against the worst possible scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-case-risk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate worst-case risk for each estimator\n",
    "n = 16\n",
    "p_grid = np.linspace(0.001, 0.999, 1000)\n",
    "\n",
    "worst_case = {\n",
    "    'MLE': np.max(mse_mle(p_grid, n)),\n",
    "    'Laplace +1/+1': np.max(mse_laplace1(p_grid, n)),\n",
    "    'Laplace +2/+2': np.max(mse_laplace2(p_grid, n)),\n",
    "    'Bad': np.max(mse_bad(p_grid, n))\n",
    "}\n",
    "\n",
    "print(\"Worst-Case Risk (Maximum MSE over all p)\")\n",
    "print(\"=\" * 50)\n",
    "for name, wc in worst_case.items():\n",
    "    print(f\"{name:<20}: {wc:.6f}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nMinimax estimator: Laplace +2/+2 (smallest worst-case risk)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimax-discussion",
   "metadata": {},
   "source": [
    "For $n = 16$, the **Laplace +2/+2 estimator** $\\tilde{p}_2 = (X+2)/(n+4)$ is the minimax estimator!\n",
    "\n",
    "Its MSE curve is relatively flat, with its maximum at $p = 0.5$. By shrinking toward the center, it avoids extremely bad performance anywhere.\n",
    "\n",
    "### Average-Case Risk\n",
    "\n",
    "Instead of worst-case, we could consider **average-case** risk:\n",
    "$$R_{\\text{avg}}(T) = \\int_0^1 \\text{MSE}_p(T) \\, dp$$\n",
    "\n",
    "This averages the MSE uniformly over all possible values of $p$.\n",
    "\n",
    "**Question:** We've considered worst-case and average-case risk. Why not **best-case** risk?\n",
    "\n",
    "**Answer:** Every estimator can achieve arbitrarily small best-case risk — just find some $p$ where it happens to do well (e.g., the constant estimator $T \\equiv c$ has MSE = 0 at $p = c$). Best-case risk doesn't discriminate between estimators, so it's not useful.\n",
    "\n",
    "Which estimator minimizes average-case risk? Let's find out.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bayes-derivation",
   "metadata": {},
   "source": [
    "## 6. Minimizing Average-Case Risk: A Surprising Answer\n",
    "\n",
    "### The Problem\n",
    "\n",
    "We want to find the estimator $T(X)$ that minimizes:\n",
    "$$R_{\\text{avg}}(T) = \\int_0^1 \\text{MSE}_p(T) \\, dp = \\int_0^1 E_p[(T(X) - p)^2] \\, dp$$\n",
    "\n",
    "### Rewriting the Objective\n",
    "\n",
    "Let's interchange the integral and expectation (justified by Fubini's theorem):\n",
    "$$R_{\\text{avg}}(T) = \\int_0^1 E_p[(T(X) - p)^2] \\, dp = E\\left[(T(X) - p)^2\\right]$$\n",
    "\n",
    "where the expectation is over the **joint distribution** of $(X, p)$ with:\n",
    "- $p \\sim \\text{Uniform}(0, 1)$\n",
    "- $X \\mid p \\sim \\text{Binomial}(n, p)$\n",
    "\n",
    "### Finding the Optimal $T(X)$\n",
    "\n",
    "For any fixed value of $X = x$, what choice of $T(x)$ minimizes $E[(T(x) - p)^2 \\mid X = x]$?\n",
    "\n",
    "This is a familiar problem from probability: the value that minimizes expected squared error is the **conditional mean**:\n",
    "$$T^*(x) = E[p \\mid X = x]$$\n",
    "\n",
    "So the optimal estimator is $T^*(X) = E[p \\mid X]$.\n",
    "\n",
    "### Computing $E[p \\mid X]$\n",
    "\n",
    "We need the conditional distribution of $p$ given $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posterior-derivation",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Click to expand: Full derivation of the conditional distribution</b></summary>\n",
    "\n",
    "We compute the conditional distribution using Bayes' rule.\n",
    "\n",
    "**Joint distribution:**\n",
    "\n",
    "For $p \\in [0, 1]$ and $x \\in \\{0, 1, \\ldots, n\\}$:\n",
    "$$f(x, p) = P(X = x \\mid p) \\cdot f(p) = \\binom{n}{x} p^x (1-p)^{n-x} \\cdot 1$$\n",
    "\n",
    "**Marginal distribution of $X$:**\n",
    "\n",
    "$$P(X = x) = \\int_0^1 \\binom{n}{x} p^x (1-p)^{n-x} dp$$\n",
    "\n",
    "This integral involves the **Beta function**. Recall that:\n",
    "$$B(a, b) = \\int_0^1 p^{a-1}(1-p)^{b-1} dp = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)} = \\frac{(a-1)!(b-1)!}{(a+b-1)!}$$\n",
    "\n",
    "for positive integers $a, b$.\n",
    "\n",
    "So:\n",
    "$$P(X = x) = \\binom{n}{x} \\int_0^1 p^x (1-p)^{n-x} dp = \\binom{n}{x} B(x+1, n-x+1)$$\n",
    "\n",
    "$$= \\binom{n}{x} \\frac{x!(n-x)!}{(n+1)!} = \\frac{n!}{x!(n-x)!} \\cdot \\frac{x!(n-x)!}{(n+1)!} = \\frac{1}{n+1}$$\n",
    "\n",
    "**Interesting!** Under a uniform prior on $p$, the marginal distribution of $X$ is uniform on $\\{0, 1, \\ldots, n\\}$.\n",
    "\n",
    "**Conditional distribution of $p$ given $X$:**\n",
    "\n",
    "$$f(p \\mid x) = \\frac{f(x, p)}{P(X = x)} = \\frac{\\binom{n}{x} p^x (1-p)^{n-x}}{1/(n+1)} = (n+1)\\binom{n}{x} p^x (1-p)^{n-x}$$\n",
    "\n",
    "This is proportional to $p^x (1-p)^{n-x}$, which is the kernel of a **Beta distribution**:\n",
    "$$p \\mid X = x \\sim \\text{Beta}(x+1, n-x+1)$$\n",
    "\n",
    "**Conditional mean:**\n",
    "\n",
    "The mean of a $\\text{Beta}(\\alpha, \\beta)$ distribution is $\\frac{\\alpha}{\\alpha + \\beta}$.\n",
    "\n",
    "So:\n",
    "$$E[p \\mid X] = \\frac{X + 1}{(X+1) + (n-X+1)} = \\frac{X + 1}{n + 2}$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "punchline",
   "metadata": {},
   "source": [
    "### The Punchline\n",
    "\n",
    "The estimator that minimizes average-case risk $\\int_0^1 \\text{MSE}_p(T) \\, dp$ is:\n",
    "\n",
    "$$T^*(X) = E[p \\mid X] = \\frac{X + 1}{n + 2}$$\n",
    "\n",
    "**This is exactly Laplace's estimator $\\tilde{p}_1$!**\n",
    "\n",
    "Laplace's \"add one head and one tail\" rule, which seemed like an intuitive hack, is actually the *optimal* estimator if we measure quality by average MSE over $p \\in [0, 1]$.\n",
    "\n",
    "### The Bayesian Interpretation\n",
    "\n",
    "What we just computed has a name: it's the **posterior mean** of $p$ given $X$.\n",
    "\n",
    "- The distribution $p \\sim \\text{Uniform}(0, 1)$ is called the **prior distribution** — it represents our beliefs about $p$ before seeing data\n",
    "- The distribution $p \\mid X \\sim \\text{Beta}(X+1, n-X+1)$ is called the **posterior distribution** — it represents our updated beliefs after seeing data\n",
    "- The posterior mean $E[p \\mid X]$ is the **Bayes estimator** for squared error loss\n",
    "\n",
    "You've already seen Beta-Binomial conjugacy in your probability class — this is the same calculation! The uniform distribution $\\text{Uniform}(0,1)$ is the same as $\\text{Beta}(1, 1)$, and after observing $X$ successes in $n$ trials, the posterior is $\\text{Beta}(1 + X, 1 + n - X) = \\text{Beta}(X+1, n-X+1)$.\n",
    "\n",
    "**An estimator that minimizes average risk (with respect to some prior distribution) is called a Bayes estimator.**\n",
    "\n",
    "### What About $\\tilde{p}_2$?\n",
    "\n",
    "The Laplace +2/+2 estimator $\\tilde{p}_2 = (X+2)/(n+4)$ is also a Bayes estimator — for the **Beta(2, 2) prior** instead of the uniform prior.\n",
    "\n",
    "The Beta(2,2) prior puts more weight near $p = 0.5$ and less weight near the extremes. If we believe $p$ is likely to be moderate, this prior (and its corresponding Bayes estimator) makes sense.\n",
    "\n",
    "**Different priors lead to different Bayes estimators, each optimal for its own average-case criterion.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-risk-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average-case risk for each estimator\n",
    "from scipy import integrate\n",
    "\n",
    "n = 16\n",
    "\n",
    "# Average risk = integral of MSE over p from 0 to 1\n",
    "avg_risk_mle, _ = integrate.quad(lambda p: mse_mle(p, n), 0, 1)\n",
    "avg_risk_lap1, _ = integrate.quad(lambda p: mse_laplace1(p, n), 0, 1)\n",
    "avg_risk_lap2, _ = integrate.quad(lambda p: mse_laplace2(p, n), 0, 1)\n",
    "avg_risk_bad, _ = integrate.quad(lambda p: mse_bad(p, n), 0, 1)\n",
    "\n",
    "print(\"Average-Case Risk (Mean MSE over uniform p)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'MLE':<20}: {avg_risk_mle:.6f}\")\n",
    "print(f\"{'Laplace +1/+1':<20}: {avg_risk_lap1:.6f}  <-- Minimum!\")\n",
    "print(f\"{'Laplace +2/+2':<20}: {avg_risk_lap2:.6f}\")\n",
    "print(f\"{'Bad':<20}: {avg_risk_bad:.6f}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nLaplace +1/+1 minimizes average-case risk (it's the Bayes estimator for uniform prior)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simulation",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Simulation: Sampling Distributions of the Estimators\n",
    "\n",
    "Let's visualize how the different estimators behave by simulating their sampling distributions for a specific true value of $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sampling-distributions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate sampling distributions for different true values of p\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 16\n",
    "n_sims = 10000\n",
    "true_p_values = [0.3, 0.5, 0.7]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, true_p in zip(axes, true_p_values):\n",
    "    # Simulate X ~ Binomial(n, true_p)\n",
    "    X = np.random.binomial(n, true_p, size=n_sims)\n",
    "    \n",
    "    # Compute estimates\n",
    "    est_mle = X / n\n",
    "    est_lap1 = (X + 1) / (n + 2)\n",
    "    est_lap2 = (X + 2) / (n + 4)\n",
    "    \n",
    "    # Plot histograms\n",
    "    bins = np.linspace(0, 1, 30)\n",
    "    ax.hist(est_mle, bins=bins, alpha=0.5, color=COLOR_MLE, \n",
    "            label=f'MLE (MSE={np.mean((est_mle - true_p)**2):.4f})', density=True)\n",
    "    ax.hist(est_lap1, bins=bins, alpha=0.5, color=COLOR_LAPLACE1,\n",
    "            label=f'+1/+1 (MSE={np.mean((est_lap1 - true_p)**2):.4f})', density=True)\n",
    "    ax.hist(est_lap2, bins=bins, alpha=0.5, color=COLOR_LAPLACE2,\n",
    "            label=f'+2/+2 (MSE={np.mean((est_lap2 - true_p)**2):.4f})', density=True)\n",
    "    \n",
    "    # Mark true value\n",
    "    ax.axvline(true_p, color=COLOR_TRUE, linestyle='--', linewidth=2, label=f'True p = {true_p}')\n",
    "    \n",
    "    ax.set_xlabel('Estimate', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title(f'True p = {true_p}', fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "plt.suptitle(f'Sampling Distributions of Estimators (n = {n}, {n_sims:,} simulations)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simulation-observations",
   "metadata": {},
   "source": [
    "### Observations from the Simulation\n",
    "\n",
    "- **At $p = 0.5$**: The Laplace estimators have lower MSE than the MLE. Their shrinkage toward 0.5 helps because 0.5 is the truth!\n",
    "\n",
    "- **At $p = 0.3$ and $p = 0.7$**: The MLE and Laplace estimators are more comparable. The Laplace estimators are biased toward 0.5, which slightly hurts them here.\n",
    "\n",
    "- **All three admissible estimators** concentrate around the true value, but with different bias-variance tradeoffs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Definition |\n",
    "|---------|------------|\n",
    "| **Loss function** | $L(\\theta, a)$ — measures how bad estimate $a$ is when truth is $\\theta$ |\n",
    "| **Risk function** | $R(\\theta, T) = E_\\theta[L(\\theta, T(X))]$ — expected loss |\n",
    "| **MSE** | $E_\\theta[(T(X) - \\theta)^2] = \\text{Bias}^2 + \\text{Variance}$ |\n",
    "| **Admissible** | Not dominated by any other estimator |\n",
    "| **Minimax** | Minimizes worst-case risk $\\max_\\theta R(\\theta, T)$ |\n",
    "| **Bayes estimator** | Minimizes average risk $\\int R(\\theta, T) \\pi(\\theta) d\\theta$ |\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **The MLE is not always the best estimator.** It's the best *unbiased* estimator, but biased estimators can have lower MSE.\n",
    "\n",
    "2. **Shrinkage can help.** The Laplace estimators shrink toward 0.5, trading bias for reduced variance. This helps when the true $p$ is near 0.5.\n",
    "\n",
    "3. **Admissibility is a minimal requirement.** The bad estimator $(X+1)/n$ is inadmissible — strictly dominated by the MLE. But many admissible estimators exist.\n",
    "\n",
    "4. **No uniformly best estimator exists.** Different estimators win for different values of $p$.\n",
    "\n",
    "5. **Optimality criteria help us choose:**\n",
    "   - **Minimax**: Laplace +2/+2 minimizes worst-case risk\n",
    "   - **Bayes (uniform prior)**: Laplace +1/+1 minimizes average-case risk\n",
    "\n",
    "6. **The Bayes estimator is the posterior mean.** This is our first glimpse of Bayesian statistics — arrived at through a purely frequentist argument!\n",
    "\n",
    "### Next Time\n",
    "\n",
    "In Lecture 7, we'll develop the **Bayesian perspective** more fully:\n",
    "- Treat $\\theta$ as a random variable with a **prior distribution**\n",
    "- Update to a **posterior distribution** after seeing data\n",
    "- Understand why the **likelihood** is all that matters (given the prior)\n",
    "- Explore the asymptotic behavior of the posterior\n",
    "\n",
    "### Worksheet 3 Preview\n",
    "\n",
    "Problem 5 explores **scale-equivariant estimation** and proves that unbiased equivariant estimators are always inadmissible — you can always improve by shrinking! This complements today's theme: unbiasedness is not always a virtue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
