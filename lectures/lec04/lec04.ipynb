{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Lecture 4: Asymptotic Normality of the MLE\n",
    "\n",
    "**Data 145, Spring 2026: Evidence and Uncertainty**  \n",
    "**Instructors:** Ani Adhikari, William Fithian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26efe286-4d68-46f8-b937-427eeb571c21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As in Lecture 3, **our focus is on single-parameter models** where the distribution is known up to one real number $\\theta$.\n",
    "\n",
    "The goal is to estimate $\\theta$ by the method of maximum likelihood, and to examine the properties of the estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023b106-9cca-45fe-a354-11ee94b36f06",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Road Map for the Lecture\n",
    "\n",
    "1. Recall what we know about MLEs in particular models, and notice a common theme.\n",
    "2. Describe the common theme as a property of maximum likelihood estimation instead of something that has to be derived separately for each model.\n",
    "3. Define some useful quantities and check that they behave the way we expect them to in known cases.\n",
    "4. Derive the main properties – consistency, asymptotic normality – of the MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f531e74c-0697-43cc-b360-fe4e923cf4f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. A Recurring Theme\n",
    "\n",
    "Let the sample $X_1, X_2, \\ldots, X_n$ be i.i.d. with the distribution in the first column below. Let $\\bar{X}_n$ be the sample mean. In Lecture 1 and your probabiilty class, you established the asymptotic normality of the MLE of a parameter or parameters in each distribution, but your reasoning varied depending on the model.\n",
    "\n",
    "Distribution | Parameter | MLE | Reason for Asymptotic Normality of the MLE\n",
    ":------------|:---------|:---|:------------------------------------------|\n",
    "Exponential  | Rate $\\lambda$| $1/\\bar{X}_n$ | Delta method, starting with the CLT applied to $\\bar{X}_n$ |\n",
    "Bernoulli | Success probability $p$ | $\\bar{X}_n$ | CLT |\n",
    "Poisson | Mean $\\mu$ | $\\bar{X}_n$ | CLT |\n",
    "Normal (unknown $\\mu$, $\\sigma$) | Mean $\\mu$ | $\\bar{X}_n$ | Normal for all $n$ |\n",
    "Normal (unknown $\\mu$, $\\sigma$) | Variance $\\sigma^2$ | $\\hat{\\sigma}^2 = \\displaystyle \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X}_n)^2$ | $\\displaystyle \\frac{n}{\\sigma^2}{\\hat{\\sigma}^2} \\sim \\chi_{n-1}^2$, roughly normal for large $n$\n",
    "\n",
    "When there's a recurring result like the asymptotic normality we discovered in all these cases, it's worth trying to see if there's a more general reason for it than the argument we've given in each particular case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b70f7-b7cc-4796-a978-44f6ce22e978",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Describing the General Result\n",
    "\n",
    "Let $X_1, X_2, \\ldots, $ be i.i.d. with a \"nice\" distribution that has a parameter $\\theta$ whose unknown true value is $\\theta_0$. Later we'll state a couple of ways in which distributions are \"nice\". These are regularity conditions under which we can prove our results. All the distributions in the table above are nice.\n",
    "\n",
    "The main result is that the distribution of an MLE is asymptotically normal. We don't have to come up with a different argument for asymptotic normality for each underlying distribution. \n",
    "\n",
    "We will prove (or almost-prove) the result in this lecture and Lecture 5. For now we'll just state it and see how it can be used.\n",
    "\n",
    "#### Asymptotic Normality of the MLE\n",
    "\n",
    "Let $\\hat{\\theta}_n$ be the MLE of $\\theta_0$ based on $X_1, X_2, \\ldots, X_n$. Then\n",
    "$$\n",
    "\\sqrt{n} (\\hat{\\theta}_n - \\theta_0) \\stackrel{d}{\\to} N(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "We'll say more about $\\sigma^2$ shortly. For now, remember that using the result above means understanding it as follows:\n",
    "\n",
    "$\\hat{\\theta}_n$ is approximately normal $(\\theta_0, \\sigma^2/n)$ for large $n$.\n",
    "\n",
    "This says that the MLE based on a large sample is approximately normal, centered at the true value of the parameter, with an SD decreasing like $1/\\sqrt{n}$. \n",
    "\n",
    "#### Confidence Interval for $\\theta_0$\n",
    "\n",
    "By the normal approximation, for large $n$ we have\n",
    "$$\n",
    "P_{\\theta_0} (\\vert \\hat{\\theta}_n - \\theta_0 \\vert > 2\\displaystyle{\\frac{\\sigma}{\\sqrt{n}}}) \\approx 0.95\n",
    "$$\n",
    "\n",
    "This allows us to build confidence intervals for $\\theta_0$ in the same way you made confidence intervals for a population mean: Start at the estimate and go 2 SDs on either side.\n",
    "\n",
    "Thus the interval $\\hat{\\theta}_n \\pm 2\\displaystyle \\frac{\\sigma}{\\sqrt{n}}$ is an approximate 95\\% confidence interval for $\\theta_0$.\n",
    "\n",
    "Also as before, if you want a confidence level different from 95%, you can replace the factor of $2$ by the appropriate value of $z$.\n",
    "\n",
    "#### But What is $\\sigma^2$?\n",
    "\n",
    "That's the burning question. Here is a preview of what we will discover about $\\sigma^2$ in this lecture and the next. Then we'll roll up our sleeves and start doing the details.\n",
    "\n",
    "- $\\sigma^2$ is a positive number that depends on $\\theta_0$.\n",
    "- $\\sigma^2 = \\displaystyle \\frac{1}{I(\\theta_0)}$ where $I$ is a function that we will define. $I(\\theta)$ will be called the Fisher information of a single observation, evaluated at $\\theta$. We'll discuss the reason for the name.\n",
    "\n",
    "Since $I(\\theta_0)$ depends on $\\theta_0$, we won't be able to calculate it exactly. But, in a move that should feel familiar, we will replace it by $I(\\hat{\\theta}_n)$ which we can compute based on the sample.  The asymptotic normality will still hold, just as it did in when you replaced the population SD by the sample SD in confidence intervals for the population mean (see Problem 5 of Worksheet 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e5d289-3b47-4929-bdb4-46b8f9f85e49",
   "metadata": {},
   "source": [
    "We will now define some quantities we'll need to derive asymptotic normality. The starting point is the familiar pair of likelihood and log-likelihood."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7220009a-0bc8-40c7-b70f-311394a07698",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.1. Deriving Asymptotic Normality: Terminology and Notation Recap\n",
    "\n",
    "#### Likelihood \n",
    "\n",
    "Given i.i.d. data $X_1, \\ldots, X_n$ from density $f_\\theta$, the **likelihood function** is:\n",
    "\n",
    "$$\\text{Lik}(\\theta; X) = \\prod_{i=1}^n f_\\theta(X_i)$$\n",
    "\n",
    "This is the joint density of the data, viewed as a function of $\\theta$ (with data held fixed).\n",
    "\n",
    "#### Log-Likelihood\n",
    "\n",
    "The **log-likelihood** is:\n",
    "\n",
    "$$\\ell_n(\\theta; X) = \\log \\text{Lik}(\\theta; X) = \\sum_{i=1}^n \\log f_\\theta(X_i)$$\n",
    "\n",
    "The subscript $n$ reminds us that $X$ is a sample of size $n$.\n",
    "\n",
    "#### Maximum Likelihood Estimator\n",
    "\n",
    "The **maximum likelihood estimator** (MLE) is:\n",
    "\n",
    "$$\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_{\\theta \\in \\Theta} \\text{Lik}(\\theta; X) = \\arg\\max_{\\theta \\in \\Theta} \\ell_n(\\theta; X)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbc19777-9a93-4906-a625-0f7290237216",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.2. The Score Function\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "The MLE $\\hat{\\theta}$ is (typically) found by setting the derivative of the log-likelihood to zero:\n",
    "\n",
    "$$\\frac{d}{d\\theta} \\ell_n(\\theta; X) \\bigg|_{\\theta = \\hat{\\theta}_{MLE}} = 0$$\n",
    "\n",
    "To understand the MLE more deeply — especially its sampling distribution — we need to study the derivative of the log-likelihood.\n",
    "\n",
    "#### Definition: The Score\n",
    "\n",
    "The **score function** (or just **score**) is the derivative of the log-likelihood with respect to $\\theta$:\n",
    "\n",
    "$$S_n(\\theta; X) = \\ell_n'(\\theta; X) = \\frac{\\partial}{\\partial \\theta} \\ell_n(\\theta; X)$$\n",
    "\n",
    "For an i.i.d. model, let $\\ell_1(\\theta; X_i) = \\log f_\\theta(X_i)$ denote the log-likelihood contribution from the single observation $X_i$. Then:\n",
    "\n",
    "$$\\ell_n(\\theta; X) = \\sum_{i=1}^n \\ell_1(\\theta; X_i)$$\n",
    "\n",
    "and the score decomposes as:\n",
    "$$S_n(\\theta; X) = \\ell_n'(\\theta; X) = \\sum_{i=1}^n \\ell_1'(\\theta; X_i)$$\n",
    "\n",
    "The score is a sum of i.i.d. terms! The CLT will enter the picture at some point. So we'll need the mean and variance:\n",
    "\n",
    "$$E_\\theta (S_n(\\theta; X)) = nE_\\theta(\\ell_1'(\\theta; X_1))$$\n",
    "\n",
    "$$Var_\\theta (S_n(\\theta; X)) = nVar_\\theta(\\ell_1'(\\theta; X_1))$$\n",
    "\n",
    "**Important:** We're differentiating with respect to $\\theta$, not with respect to $X_i$. Even if $X_i$ is discrete, the score is well-defined as long as $f_\\theta(x)$ is differentiable in $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078773e1-04a7-4c3c-b2ce-4112916f6a9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Example 1: Exponential\n",
    "\n",
    "For **one observation** $X_i \\sim \\text{Exponential}(\\lambda)$: $f_\\lambda(x) = \\lambda e^{-\\lambda x}$\n",
    "\n",
    "$$\\ell_1(\\lambda; X_i) = \\log \\lambda - \\lambda X_i$$\n",
    "\n",
    "$$\\ell_1'(\\lambda; X_i) = \\frac{1}{\\lambda} - X_i$$\n",
    "\n",
    "**Confirm the MLE:**\n",
    "\n",
    "$$S_n(\\lambda; X) = \\ell_n'(\\lambda; X) = \\frac{n}{\\lambda} - \\sum_{i=1}^n X_i = \\frac{n}{\\lambda} - n\\bar{X}_n$$\n",
    "\n",
    "Setting $S_n(\\lambda; X) = 0$ gives $\\hat{\\lambda} = 1/\\bar{X}_n$, confirming our MLE.\n",
    "\n",
    "**Mean of the Score**\n",
    "$$E_\\lambda(\\ell_1'(\\lambda; X_i)) = E_\\lambda(\\frac{1}{\\lambda} - X_i) = \\frac{1}{\\lambda} - \\frac{1}{\\lambda} = 0$$\n",
    "\n",
    "So $E(S_n(\\lambda; X)) = 0$.\n",
    "\n",
    "#### Example 2: Gaussian (known variance)\n",
    "\n",
    "For $X_i \\sim N(\\mu, \\sigma^2)$ with $\\sigma^2$ known: $f(x) = \\displaystyle \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2}\\left( \\frac{x-\\mu}{\\sigma} \\right)^2}$\n",
    "\n",
    "$$\\ell_1(\\mu; X_i) = -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(X_i-\\mu)^2}{2\\sigma^2}$$\n",
    "\n",
    "$$\\ell_1'(\\mu; X_i) = \\frac{X_i - \\mu}{\\sigma^2}$$\n",
    "\n",
    "Once again, the mean of the score is 0: $E_\\mu(S_n(\\mu; X)) = 0$.\n",
    "\n",
    "#### Example 3: Bernoulli\n",
    "\n",
    "For $X_i \\sim \\text{Bernoulli}(p)$: $f_p(x) = p^x(1-p)^{1-x}$ for $x \\in \\{0, 1\\}$\n",
    "\n",
    "$$\\ell_1(p; X_i) = X_i \\log p + (1-X_i) \\log(1-p)$$\n",
    "\n",
    "$$\\ell_1'(p; X_i) = \\frac{X_i}{p} - \\frac{1-X_i}{1-p} = \\frac{X_i - p}{p(1-p)}$$\n",
    "\n",
    "Yet again, the mean of the score is 0: $E_p(S_n(p; X)) = 0$.\n",
    "\n",
    "Yes, there's a pattern here, and we can generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f11afdf-c3d7-4be6-9083-03e7aece55a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.3. Mean of the Score\n",
    "\n",
    "A fundamental fact: **the score has mean zero at the value of the parameter.** That is,\n",
    "\n",
    "**Theorem:** $E_{\\theta}[\\ell_1'(\\theta; X_i)] = 0$\n",
    "\n",
    "For this result to hold, the $\\theta$ with respect to which the expectation is taken must be the same as the $\\theta$ at which $\\ell_1$ is evaluated.\n",
    "\n",
    "**Proof:** We can write:\n",
    "$$\\ell_1'(\\theta; X_i) = \\frac{\\partial}{\\partial\\theta} \\log f_\\theta(X_i) = \\frac{\\frac{\\partial}{\\partial\\theta} f_\\theta(X_i)}{f_\\theta(X_i)}$$\n",
    "\n",
    "Taking the expectation (integrating over the $\\theta$-distribution of $X_i$):\n",
    "$$E_\\theta[\\ell_1'(\\theta; X_i)] = \\int \\frac{\\frac{\\partial}{\\partial\\theta} f_\\theta(x)}{f_\\theta(x)} \\cdot f_\\theta(x) \\, dx = \\int \\frac{\\partial}{\\partial\\theta} f_\\theta(x) \\, dx$$\n",
    "\n",
    "Now we interchange the derivative (with respect to $\\theta$) and the integral (with respect to $x$):\n",
    "$$\\int \\frac{\\partial}{\\partial\\theta} f_\\theta(x) \\, dx = \\frac{\\partial}{\\partial\\theta} \\int f_\\theta(x) \\, dx = \\frac{\\partial}{\\partial\\theta} (1) = 0 \\quad \\square$$\n",
    "\n",
    "(The interchange is valid under regularity conditions that hold for \"nice\" models.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fisher-info",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.4. The Fisher Information\n",
    "\n",
    "Since the score has mean zero, its variance is particularly important.\n",
    "\n",
    "#### Definition\n",
    "\n",
    "The **Fisher information** in a single observation $X_i$ is defined by:\n",
    "\n",
    "$$I(\\theta) = Var_\\theta(\\ell_1'(\\theta; X_i)) = E_\\theta(\\ell_1'(\\theta; X_i)^2)$$\n",
    "\n",
    "The second equality uses $E_\\theta(\\ell_1'(\\theta; X_i)) = 0$.\n",
    "\n",
    "For $n$ i.i.d. observations, the **total Fisher information** is $nI(\\theta)$, since:\n",
    "$$Var_\\theta(S_n(\\theta; X)) = n \\cdot Var_\\theta(\\ell_1'(\\theta; X_i)) = nI(\\theta)$$\n",
    "\n",
    "#### Why \"Information\"?\n",
    "\n",
    "Intuitively, the Fisher information measures **how much the data tells us about $\\theta$**:\n",
    "- High $I(\\theta)$ means $\\ell_1'$ varies a lot → small changes in $\\theta$ produce large changes in the likelihood → data is informative about $\\theta$\n",
    "- Low $I(\\theta)$ means the likelihood is flat → data doesn't distinguish well between different $\\theta$ values\n",
    "\n",
    "We'll see this more precisely next lecture: the variance of the MLE is approximately $1/(nI(\\theta))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4daebe1-dfa4-4778-9c07-74c66262d5e7",
   "metadata": {},
   "source": [
    "#### Fisher Information Examples\n",
    "\n",
    "**Exponential:** $\\ell_1'(\\lambda; X_i) = 1/\\lambda - X_i$, where $E_\\lambda(X_i) = 1/\\lambda$, $\\text{Var}_\\lambda(X_i) = 1/\\lambda^2$\n",
    "\n",
    "$$I(\\lambda) = Var_\\lambda\\left(\\frac{1}{\\lambda} - X_i\\right) = Var_\\lambda(X_i) = \\frac{1}{\\lambda^2}$$\n",
    "\n",
    "**Gaussian:** $\\ell_1'(\\mu; X_i) = (X_i - \\mu)/\\sigma^2$\n",
    "\n",
    "$$I(\\mu) = Var_\\mu\\left(\\frac{X_i - \\mu}{\\sigma^2}\\right) = \\frac{Var_\\mu(X_i)}{\\sigma^4} = \\frac{\\sigma^2}{\\sigma^4} = \\frac{1}{\\sigma^2}$$\n",
    "\n",
    "**Bernoulli:** $\\ell_1'(p; X_i) = (X_i - p)/(p(1-p))$\n",
    "\n",
    "$$I(p) = Var_p\\left(\\frac{X_i - p}{p(1-p)}\\right) = \\frac{Var_p(X_i)}{[p(1-p)]^2} = \\frac{p(1-p)}{[p(1-p)]^2} = \\frac{1}{p(1-p)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0616912-89ca-41de-80f8-27714bcb2a92",
   "metadata": {},
   "source": [
    "#### Agrees with Our Claim about Asymptotic Normality\n",
    "\n",
    "In all three cases, $I(\\theta) = 1/Var_\\theta(X_i)$. This is not a coincidence — it holds for nice one-parameter families called \"exponential families\"!\n",
    "\n",
    "But for our purposes, the crucial pattern is that these values make sense in the main result we stated earlier (and have yet to prove):\n",
    "\n",
    "**The MLE $\\hat{\\theta}_n$ is approximately normal $(\\theta_0, \\sigma^2/n)$ for large $n$. Here $\\sigma^2 = 1/I(\\theta_0)$.**\n",
    "\n",
    "Let's see if this checks out for the Gaussian and the Bernoulli. The exponential is for you to check in Worksheet 2.\n",
    "\n",
    "**Asymptotic normality, Gaussian Case:** This one is true without the word \"asymptotic\". The true value of the parameter is $\\mu_0$ and the MLE is $\\bar{X}_n$. You know from your probability class that this MLE is normal for all $n$. For each $n$ it has mean $\\mu_0$ and variance $\\sigma^2/n = \\displaystyle \\frac{1}{n(1/\\sigma^2)} = \\frac{1}{nI(\\mu_0)}$. ✓\n",
    "\n",
    "**Asymptotic normality, Bernoulli Case:** The true value of the parameter is $p_0$ and the MLE is the sample proportion $\\bar{X}_n$. By the CLT, this is asymptotically normal. It is also unbiased, so its mean is $p_0$. Its variance is $\\displaystyle \\frac{p_0(1-p_0)}{n} = \\frac{1}{n(1/p_0(1-p_0))} = \\frac{1}{nI(p_0)}$. ✓"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836e146e-ff22-4112-b170-327b6edb40eb",
   "metadata": {},
   "source": [
    "## In class, Lecture 4 stopped here. Lecture 5 will start with 3.5 below.\n",
    "\n",
    "But at this point, the content below should be a relatively easy read. Try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fisher-second-deriv",
   "metadata": {},
   "source": [
    "### 3.5. An Alternative Formula for the Fisher Information\n",
    "\n",
    "There's another way to compute Fisher information that's often more convenient for calculations. \n",
    "\n",
    "Assume that $\\ell$ is twice differentiable.\n",
    "\n",
    "**Theorem:** $I(\\theta) = -E_\\theta[\\ell_1''(\\theta; X_i)]$\n",
    "\n",
    "**Proof:** We showed that $E_\\theta[\\ell_1'(\\theta; X_i)] = 0$ for all $\\theta$. Differentiate both sides with respect to $\\theta$:\n",
    "\n",
    "$$0 = \\frac{\\partial}{\\partial\\theta} E_\\theta[\\ell_1'(\\theta; X_i)] = \\frac{\\partial}{\\partial\\theta} \\int \\ell_1'(\\theta; x) f_\\theta(x) \\, dx$$\n",
    "\n",
    "Switch the derivative and integral, and use the product rule of derivatives:\n",
    "$$0 = \\int \\ell_1''(\\theta; x) \\cdot f_\\theta(x) \\, dx + \\int \\ell_1'(\\theta; x) \\cdot \\frac{\\partial}{\\partial\\theta} f_\\theta(x) \\, dx$$\n",
    "\n",
    "The first integral is $E_\\theta[\\ell_1''(\\theta; X_i)]$. For the second, note that $\\frac{\\partial}{\\partial\\theta} f_\\theta(x) = \\ell_1'(\\theta; x) \\cdot f_\\theta(x)$, so:\n",
    "$$\\int \\ell_1'(\\theta; x) \\cdot \\frac{\\partial}{\\partial\\theta} f_\\theta(x) \\, dx = \\int \\ell_1'(\\theta; x)^2 f_\\theta(x) \\, dx = E_\\theta[\\ell_1'(\\theta; X_i)^2] = I(\\theta)$$\n",
    "\n",
    "Therefore: $0 = E_\\theta[\\ell_1''(\\theta; X_i)] + I(\\theta)$, giving $I(\\theta) = -E_\\theta[\\ell_1''(\\theta; X_i)]$. $\\square$\n",
    "\n",
    "**Interpretation:** The Fisher information equals the negative expected curvature of the log-likelihood. More curvature at the maximum means the MLE is more precisely determined.\n",
    "\n",
    "#### Check the New Formula in the Normal Case\n",
    "If the sample is i.i.d. normal $(\\mu, \\sigma^2)$ for a known $\\sigma^2$, we have seen that\n",
    "$$\n",
    "\\ell_1'(\\mu; X_i) = \\frac{X_i - \\mu}{\\sigma^2}\n",
    "$$\n",
    "so\n",
    "$$\n",
    "\\ell_1''(\\mu; X_i) = -\\frac{1}{\\sigma^2}\n",
    "$$\n",
    "Note that **this is a constant** so its expectation is just itself, and it agrees with $Var_\\mu(\\ell_1'(\\mu; X_i))$ calculated earlier.\n",
    "\n",
    "#### Check the New Formula in the Exponential Case\n",
    "If the sample is i.i.d. exponential with rate $\\lambda$, we have seen that\n",
    "$$\n",
    "\\ell_1'(\\lambda; X_i) = \\frac{1}{\\lambda} - X_i\n",
    "$$\n",
    "so\n",
    "$$\n",
    "\\ell_1''(\\lambda; X_i) = -\\frac{1}{\\lambda^2}\n",
    "$$\n",
    "Once again, it's a constant, so its expectation is just itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a2f89f-0304-4b0f-8b37-a63499fc4116",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.1. Towards Asymptotic Normality: Consistency\n",
    "\n",
    "This was proved in Lecture 3 (apart from some care required to establish uniform convergence instead of pointwise convergence; but don't worry about that).\n",
    "\n",
    "Let $\\hat{\\theta}_n$ be the MLE of the true parameter $\\theta_0$ based on $X_1, X_2, \\ldots, X_n$. \n",
    "\n",
    "Then $\\hat{\\theta}_n$ is a consistent estimator of $\\theta_0$. That is, $\\hat{\\theta}_n \\stackrel{P}{\\to} \\theta_0$.\n",
    "\n",
    "Here the $P$ in the $\\stackrel{P}{\\to}$ symbol is the true underlying probability distribution, that is, $P_{\\theta_0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d6d371-6c6d-40ec-8000-71b727facf8d",
   "metadata": {},
   "source": [
    "### 4.2. Towards Asymptotic Normality: Taylor Expansion\n",
    "\n",
    "The MLE is obtained by setting the derivative of the log-likelihood to be 0. Since the derivative of the log-likelihood is the score function, we have $0 = S_n(\\hat{\\theta}_n; X)$.\n",
    "\n",
    "Since $\\hat{\\theta}_n$ and the true $\\theta_0$ are likely to be close for large $n$, use a Taylor expansion of $S_n\\hat{\\theta}_n; X)$ about $\\theta_0$. For ease of notation, we will suppress the sample $X$ from now on. But it's there, and it's the reason the equalities below are equalities of random variables.\n",
    "\n",
    "$$\n",
    "0 ~\\approx~ S_n(\\hat{\\theta}_n) ~=~ S_n(\\theta_0) + (\\hat{\\theta}_n - \\theta_0)S_n'(\\tilde{\\theta}_n)\n",
    "$$\n",
    "for some point $\\tilde{\\theta}_n$ between $\\hat{\\theta}_n$ and $\\theta_0$.\n",
    "\n",
    "Note that we're assuming $S_n$ is differentiable. \n",
    "\n",
    "Rewrite the above to see that\n",
    "$$\\hat{\\theta}_n - \\theta_0 ~ = ~ \\frac{S_n(\\theta_0)}{-S_n'(\\tilde{\\theta}_n)}$$\n",
    "and hence\n",
    "$$\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) ~ = ~ \\frac{\\frac{1}{\\sqrt{n}}S_n(\\theta_0)}\n",
    "{-\\frac{1}{n}S_n'(\\tilde{\\theta}_n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c0c78-3f65-4e62-a3c6-8f55a90fde84",
   "metadata": {},
   "source": [
    "### 4.3. Towards Asymptotic Normality: The Numerator\n",
    "\n",
    "$$S_n(\\theta_0) = \\sum_{i=1}^n \\ell_1'(\\theta_0; X_i)$$\n",
    "\n",
    "This is a sum of i.i.d. random variables with common mean $0$ and common variance $I(\\theta_0)$. Here the mean and variance are calculated using the true $\\theta_0$ as the parameter.\n",
    "\n",
    "By the CLT, \n",
    "$$\\frac{S_n(\\theta_0)}{\\sqrt{nI(\\theta_0)}} \\stackrel{d}{\\to} N(0, 1)$$\n",
    "and hence\n",
    "$$\\frac{S_n(\\theta_0)}{\\sqrt{n}} \\stackrel{d}{\\to} N(0, I(\\theta_0))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db5859-084e-4bd7-b7dc-b65e1f524eea",
   "metadata": {},
   "source": [
    "### 4.4. Towards Asymptotic Normality: The Denominator\n",
    "\n",
    "First note that for any $\\theta$,\n",
    "$$\\frac{1}{n} S_n'(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\ell_1^\"(\\theta)$$\n",
    "\n",
    "This is the mean of an i.i.d. sample. By the Weak Law of Large Numbers,\n",
    "$$ \\frac{1}{n} S_n'(\\theta) \\stackrel{P}{\\to} E_{\\theta}(\\ell_1^\"(\\theta)) = -I(\\theta)$$\n",
    "\n",
    "By consistency of the MLE, $\\hat{\\theta}_n \\stackrel{P}{\\to} \\theta_0$, where the probabilities are calculated using the true $\\theta_0$.\n",
    "\n",
    "By the same \"squeezing\" argument as in the derivation of the delta method, $\\vert \\tilde{\\theta}_n - \\theta_0 \\vert \\le \\vert \\hat{\\theta}_n - \\theta_0 \\vert$ and so $\\tilde{\\theta}_n \\stackrel{P}{\\to} \\theta_0$.\n",
    "\n",
    "We want to conclude that $\\displaystyle \\frac{1}{n} S_n'(\\tilde{\\theta}) \\stackrel{P}{\\to} -I(\\theta_0)$ when the probabilities are calculated using the true $\\theta_0$. But we don't quite have that, and it takes some work and regularity conditions to prove.\n",
    "\n",
    "It's fine to simply assume that we have enough regularity to make it work, and therefore the denominator converges in probability to the constant $-I(\\theta_0)$.\n",
    "\n",
    "In fact, in all our examples we've seen that $\\ell_1''(\\theta; X_i)$ is a constant (that is, a non-random quantity) involving $\\theta$. Thich implies $\\displaystyle \\frac{1}{n} S_n'(\\tilde{\\theta}_n)$ is that same quantity for every $n$. So the \"convergence\", if we still want to call it that, is automatically to that quantity evaluated at $\\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469990c8-4c6c-4f32-aa05-35a8147a3934",
   "metadata": {},
   "source": [
    "### 4.5. Asymptotic Normality\n",
    "\n",
    "Now use Slutsky's theorem to see that $\\sqrt{n}(\\hat{\\theta}_n - \\theta_0)$ converges in distribution to a normal $(0, I(\\theta_0)$ random variable times the constant $-1/I(\\theta_0)$. That constant is squared in the calculation of the variance, so \n",
    "\n",
    "$$\n",
    "\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\stackrel{d}{\\to} N(0, 1/I(\\theta_0))\n",
    "$$\n",
    "\n",
    "as we had claimed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff75c3-e8bd-4540-a649-3bb7734de688",
   "metadata": {},
   "source": [
    "### Next Lecture\n",
    "\n",
    "- Complete the argument that the MLE is asymptotically normal with variance $1/(nI(\\theta))$\n",
    "- What does it mean to be \"efficient\"?\n",
    "- Show that the MLE is pretty much the best thing you can do, in many situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139857bc-f7b5-47dd-9d50-85bb4f7b100c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
