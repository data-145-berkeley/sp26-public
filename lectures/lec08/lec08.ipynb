{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Lecture 8: Bayesian Priors and the Meaning of Probability\n",
    "\n",
    "**Data 145, Spring 2026: Evidence and Uncertainty**  \n",
    "**Instructors:** Ani Adhikari, William Fithian\n",
    "\n",
    "---\n",
    "\n",
    "**Please run the setup cell below before reading.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.special import betaln\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "# Color scheme for Bayesian lectures (Scheme B, same as lec07)\n",
    "# Black = data (likelihood)\n",
    "# Blue = beliefs (prior and posterior; prior dashed, posterior solid)\n",
    "# Red = asymptotic approximations (BvM normal approx, dashed)\n",
    "COLOR_LIKELIHOOD = 'black'       # Data / likelihood\n",
    "COLOR_PRIOR = 'steelblue'        # Prior belief (dashed)\n",
    "COLOR_POSTERIOR = 'steelblue'    # Posterior belief (solid)\n",
    "COLOR_APPROX = 'firebrick'      # Asymptotic approximations (dashed)\n",
    "COLOR_TRUE = '#000000'           # True parameter value\n",
    "\n",
    "# Shades of blue for comparing multiple posteriors\n",
    "MULTI_BLUES = ['#1b4f72', '#2e86c1', '#5dade2', '#85c1e9']\n",
    "\n",
    "np.random.seed(145)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In Lecture 7 we developed the **Bayesian framework**: starting from a prior $\\pi(\\theta)$ and a likelihood $f_\\theta(x)$, we computed the posterior $\\pi(\\theta \\mid x) \\propto f_\\theta(x) \\cdot \\pi(\\theta)$. We saw three conjugate families (Beta-Binomial, Gamma-Exponential, Normal-Normal) and established the **Bernstein–von Mises theorem**: for large $n$, the posterior concentrates around the MLE regardless of the prior.\n",
    "\n",
    "But we also noted a word of caution: unlike the likelihood model (which we can check against data), the prior is **uncheckable**. When $n$ isn't very large — or the prior is sufficiently strong — the choice of prior can seriously affect our conclusions.\n",
    "\n",
    "### Today's roadmap\n",
    "\n",
    "Where should priors come from? What does it even mean for $\\theta$ to \"have a distribution\"? We'll explore four approaches (which are not mutually exclusive):\n",
    "\n",
    "1. **Subjective Bayes**: The prior encodes personal degrees of belief\n",
    "2. **Objective Bayes**: Flat priors and the Jeffreys prior — minimizing the influence of the prior\n",
    "3. **Convenience priors**: Conjugate priors chosen for computational tractability\n",
    "4. **Hierarchical Bayes**: Let the data from many similar problems inform the prior\n",
    "\n",
    "The hierarchical approach will motivate the **Gibbs sampler**, a computational tool for sampling from complex posterior distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec1-header",
   "metadata": {},
   "source": [
    "## 1. Motivation: Uncheckable Priors\n",
    "\n",
    "### How strong is too strong?\n",
    "\n",
    "In Lecture 7, we put three different priors on the earthquake rate $\\lambda$ — $\\text{Gamma}(1, 1)$, $\\text{Gamma}(1, 20)$, and $\\text{Gamma}(1, 365)$ — and found that the posteriors were virtually identical. That was reassuring: it suggested the prior doesn't matter.\n",
    "\n",
    "Recall from Lecture 7 that for the Gamma–Exponential model, the posterior mean of $\\lambda$ is $(\\alpha + n)/(\\beta + \\sum x_i)$. We can interpret $\\alpha$ as a \"prior sample size\" (number of pseudo-observations) and $\\beta$ as the \"prior sum of interarrival times\":\n",
    "\n",
    "- $\\text{Gamma}(1, 20)$: one pseudo-interarrival of 20 days (prior mean rate $= 0.05$/day, i.e., one earthquake every 20 days)\n",
    "- $\\text{Gamma}(1, 365)$: one pseudo-interarrival of 365 days (prior mean rate $\\approx 0.003$/day, about one per year)\n",
    "- $\\text{Gamma}(10, 3650)$: ten pseudo-interarrivals averaging 365 days — the same prior mean as $\\text{Gamma}(1, 365)$, but with ten times the \"prior sample size\"\n",
    "\n",
    "The first two priors are very weak — just a single pseudo-observation each. The third encodes the same guess about the rate as the second, but with more conviction. Below we compare all three to see whether this extra strength makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fig1-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load earthquake data (same as lec07)\n",
    "eq_data = pd.read_csv('../../demos/lec01_earthquakes/data/california_earthquakes_declustered.csv')\n",
    "mainshocks = eq_data[eq_data['is_mainshock']].sort_values('time').reset_index(drop=True)\n",
    "timestamps = pd.to_datetime(mainshocks['time'], format='ISO8601')\n",
    "interarrivals = timestamps.diff().dt.total_seconds().dropna().values / 86400\n",
    "\n",
    "n_eq = len(interarrivals)\n",
    "sum_x = np.sum(interarrivals)\n",
    "mle_lambda = 1 / np.mean(interarrivals)\n",
    "\n",
    "print(f\"Number of interarrival times: {n_eq}\")\n",
    "print(f\"MLE rate: {mle_lambda:.4f} per day\")\n",
    "\n",
    "# Three priors of varying strength\n",
    "priors = [\n",
    "    (1,   20,   'Gamma(1, 20): weak, mean 0.05',       MULTI_BLUES[0]),\n",
    "    (1,   365,  'Gamma(1, 365): weak, mean 0.003',     MULTI_BLUES[1]),\n",
    "    (10,  3650, 'Gamma(10, 3650): stronger, mean 0.003', MULTI_BLUES[2]),\n",
    "]\n",
    "\n",
    "# Print posterior means\n",
    "print(f\"\\n{'Prior':<42s} {'Prior mean':<12s} {'Post. mean':<12s}\")\n",
    "print(\"-\" * 66)\n",
    "for alpha_p, beta_p, label, _ in priors:\n",
    "    post_mean = (n_eq + alpha_p) / (beta_p + sum_x)\n",
    "    print(f\"{label:<42s} {alpha_p/beta_p:<12.4f} {post_mean:<12.4f}\")\n",
    "\n",
    "# --- Figure 1 ---\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "lam_grid = np.linspace(0.025, 0.055, 500)\n",
    "\n",
    "# Compute max posterior height for rescaling priors\n",
    "max_post_height = 0\n",
    "for alpha_p, beta_p, label, color in priors:\n",
    "    pa = n_eq + alpha_p\n",
    "    pb = beta_p + sum_x\n",
    "    pdf = stats.gamma.pdf(lam_grid, a=pa, scale=1/pb)\n",
    "    max_post_height = max(max_post_height, np.max(pdf))\n",
    "\n",
    "for alpha_p, beta_p, label, color in priors:\n",
    "    pa = n_eq + alpha_p\n",
    "    pb = beta_p + sum_x\n",
    "    post_pdf = stats.gamma.pdf(lam_grid, a=pa, scale=1/pb)\n",
    "    prior_pdf = stats.gamma.pdf(lam_grid, a=alpha_p, scale=1/beta_p)\n",
    "\n",
    "    # Rescale prior so its peak is ~25% of max posterior height\n",
    "    if np.max(prior_pdf) > 0:\n",
    "        prior_rescaled = prior_pdf * (max_post_height / np.max(prior_pdf)) * 0.25\n",
    "    else:\n",
    "        prior_rescaled = prior_pdf\n",
    "\n",
    "    ax.plot(lam_grid, prior_rescaled, color=color, linewidth=1.5, linestyle='--', alpha=0.5)\n",
    "    ax.plot(lam_grid, post_pdf, color=color, linewidth=2.5, label=label)\n",
    "\n",
    "ax.axvline(mle_lambda, color=COLOR_LIKELIHOOD, linestyle=':', linewidth=1.5,\n",
    "           label=f'MLE: {mle_lambda:.4f}')\n",
    "\n",
    "ax.set_xlabel(r'$\\lambda$ (earthquakes per day)', fontsize=11)\n",
    "ax.set_ylabel('Density', fontsize=11)\n",
    "ax.set_title('Three Priors and Their Posteriors for the Earthquake Rate',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fig1-caption",
   "metadata": {},
   "source": [
    "*__Figure 1.__ Three Gamma priors (dashed, rescaled) and their posteriors (solid) for the California earthquake rate $\\lambda$, with $n = 614$ interarrival times. The weak priors Gamma(1, 20) (dark blue) and Gamma(1, 365) (medium blue) yield virtually identical posteriors — the same reassuring result from Lecture 7. But the stronger Gamma(10, 3650) prior (light blue), which has the same mean but ten times the \"prior sample size,\" noticeably pulls the posterior to the left of the MLE (black dotted). Unlike the exponential model itself, which we can check against data, there is no diagnostic to tell us our prior is wrong.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec1-discussion",
   "metadata": {},
   "source": [
    "### The prior is uncheckable\n",
    "\n",
    "We have an intuition that the $\\text{Gamma}(10, 3650)$ prior is \"too strong\" — but how would we check? When we assessed the exponential model in Lecture 1, we could plot a histogram and compare it to the fitted density. For the prior, we have at most one \"observation\" of $\\lambda$ — and even that we don't observe directly. Come to think of it, what does it even *mean* to say $\\lambda$ has a distribution?\n",
    "\n",
    "This question leads us to ask: **where should priors come from?** We'll discuss four approaches — subjective, objective, convenience, and hierarchical priors.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec2",
   "metadata": {},
   "source": [
    "## 2. Epistemic Probability and Subjective Bayes\n",
    "\n",
    "### Two kinds of uncertainty\n",
    "\n",
    "Consider two statements about earthquakes:\n",
    "\n",
    "- \"The waiting time until the next M $\\geq$ 4 earthquake\" — this feels genuinely random (**aleatory** uncertainty)\n",
    "- \"The rate $\\lambda$ at which earthquakes arrive in California\" — this is a fixed fact about the world that we happen not to know (**epistemic** uncertainty)\n",
    "\n",
    "**Aleatory uncertainty** is uncertainty about whether something will *happen* — randomness inherent in a physical process whose outcome hasn't yet been determined. **Epistemic uncertainty** is uncertainty about whether something *is true* — the fact is already settled, we just don't know it.\n",
    "\n",
    "People argue about what kinds of things we should assign probabilities to. Here are some examples of events, ranging from most aleatory to most epistemic, that people may be more or less comfortable assigning probabilities to:\n",
    "\n",
    "| Example | Discussion |\n",
    "|---------|------------|\n",
    "| **Outcome of rolling a die** (most aleatory) | Most people agree on 1/6 regardless of philosophy — justified by repeated trials, physical symmetry, or rational indifference. |\n",
    "| **Whether a radiation treatment cures a cancer patient** | Requires identifying a comparison group. As you condition on more specific patient characteristics (stage, age, family history, genetics), the reference class shrinks until potentially only one individual remains. |\n",
    "| **Whether the Democratic candidate wins the next presidential election** | Every presidential election is a one-off event. Anyone qualified to assess it can identify important aspects that are more or less unprecedented. |\n",
    "| **Whether a subatomic particle has the mass predicted by a theory** | Not the probability of something happening, but of something being *true* about the world — a fact established since the universe's creation. |\n",
    "| **Whether P = NP** | A purely mathematical question, independent of empirical data. It may or may not be resolved in our lifetimes, yet people assign probabilities to it. |\n",
    "| **The 20th digit of $\\sqrt{2}$** (most epistemic) | A deterministic fact, verifiable by calculation — yet without time to compute, one might assign 10% probability if forced to bet. |\n",
    "\n",
    "But is the distinction really so crisp? (See the covered-coin demo in lecture.)\n",
    "\n",
    "### Subjective Bayes\n",
    "\n",
    "In the **subjective Bayesian** view, the prior $\\pi(\\theta)$ encodes a specific person's **degrees of belief** about $\\theta$ before seeing data. These beliefs can differ between people, and that's fine — the posterior $\\pi(\\theta \\mid X)$ tells *you* what *you* should believe after seeing the data, given what *you* believed before.\n",
    "\n",
    "**Pros**: The subjective approach is philosophically clean. It can incorporate genuine expertise — for example, a seismologist's beliefs about earthquake rates — and provides a coherent framework for updating those beliefs in light of data.\n",
    "\n",
    "**Cons**: If different people start with different priors, they get different posteriors. In practice, it is hard to elicit a full prior distribution from an expert. And when you write a paper, do you want the conclusion to be \"in my personal opinion, the parameter is probably this\"? Or do you want it to be, at least nominally, observer-agnostic?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec3-header",
   "metadata": {},
   "source": [
    "## 3. Objective Bayes: Flat Priors and the Jeffreys Prior\n",
    "\n",
    "### Flat priors\n",
    "\n",
    "Can we find a prior that represents \"no prior information\"? The simplest idea: use a **flat prior** $\\pi(\\theta) \\propto 1$.\n",
    "\n",
    "We've already seen this: the $\\text{Uniform}(0,1)$ prior for the Binomial, and the flat prior for the Normal mean. Flat priors are widely used and practically appealing: with a flat prior, the posterior mode equals the MLE.\n",
    "\n",
    "But flat priors have two issues:\n",
    "\n",
    "1. **Not always normalizable**: A flat prior on $(0, \\infty)$ (e.g., for an exponential rate $\\lambda$) doesn't integrate to a finite value. This isn't always fatal — the posterior can still be proper — but it requires care.\n",
    "\n",
    "2. **Not reparameterization-invariant**: This is the deeper problem. A flat prior on $\\theta$ is *not* flat on $g(\\theta)$ — it depends on the parameterization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec3-reparam",
   "metadata": {},
   "source": [
    "### The reparameterization problem\n",
    "\n",
    "Consider the Binomial model. Let $p$ be the success probability and $\\eta = \\log(p/(1-p))$ the log-odds.\n",
    "\n",
    "- A flat prior on $p$: $\\pi(p) \\propto 1$ for $p \\in (0,1)$\n",
    "- What does this imply for $\\eta$? By the change-of-variables formula: $\\pi(\\eta) = \\pi(p) \\cdot |dp/d\\eta| = \\frac{e^\\eta}{(1 + e^\\eta)^2}$\n",
    "\n",
    "<details>\n",
    "<summary><b>Recall: Change of variables for densities</b></summary>\n",
    "\n",
    "If $p$ has density $f_p(p)$ and $\\eta = g(p)$ is a monotone transformation, the density of $\\eta$ is:\n",
    "\n",
    "$$f_\\eta(\\eta) = f_p(p) \\cdot \\left|\\frac{dp}{d\\eta}\\right|$$\n",
    "\n",
    "where $p = g^{-1}(\\eta)$. **Intuition**: If $dp/d\\eta$ is large, a small interval in $\\eta$ maps to a large interval in $p$ — so probability mass is \"stretched out,\" and the density on $p$ gets *multiplied* (not divided) by $|dp/d\\eta|$.\n",
    "</details>\n",
    "\n",
    "This is the **standard logistic density** — not flat at all! It concentrates near $\\eta = 0$ (i.e., $p = 1/2$).\n",
    "\n",
    "Conversely, a flat prior on $\\eta$ would imply a non-flat, U-shaped prior on $p$.\n",
    "\n",
    "Let's see this concretely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fig2-uniform-p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Uniform-on-p prior in both parameterizations\n",
    "n_samples = 1000\n",
    "p_samples = np.random.uniform(0, 1, n_samples)\n",
    "eta_samples = np.log(p_samples / (1 - p_samples))  # logit transform\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "\n",
    "# Left panel: p-parameterization\n",
    "ax1.hist(p_samples, bins=30, density=True, color=COLOR_PRIOR, alpha=0.4,\n",
    "         edgecolor='white', label='Histogram (1000 samples)')\n",
    "ax1.axhline(1.0, color=COLOR_PRIOR, linewidth=2.5, linestyle='--',\n",
    "            label='Uniform(0,1) density')\n",
    "ax1.set_xlabel('$p$', fontsize=12)\n",
    "ax1.set_ylabel('Density', fontsize=11)\n",
    "ax1.set_title('Uniform-on-$p$ prior\\n($p$-parameterization)',\n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.legend(fontsize=9)\n",
    "\n",
    "# Right panel: eta-parameterization\n",
    "eta_grid = np.linspace(-8, 8, 500)\n",
    "logistic_density = np.exp(eta_grid) / (1 + np.exp(eta_grid))**2\n",
    "\n",
    "ax2.hist(eta_samples, bins=40, density=True, color=COLOR_PRIOR, alpha=0.4,\n",
    "         edgecolor='white', label='Histogram (1000 samples)')\n",
    "ax2.plot(eta_grid, logistic_density, color=COLOR_PRIOR, linewidth=2.5,\n",
    "         linestyle='--', label='Induced density (logistic)')\n",
    "ax2.set_xlabel(r'$\\eta = \\log(p/(1-p))$', fontsize=12)\n",
    "ax2.set_ylabel('Density', fontsize=11)\n",
    "ax2.set_title('Uniform-on-$p$ prior\\n($\\eta$-parameterization)',\n",
    "              fontsize=12, fontweight='bold')\n",
    "ax2.set_xlim(-8, 8)\n",
    "ax2.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fig2-caption",
   "metadata": {},
   "source": [
    "*__Figure 2.__ A Uniform(0, 1) prior on $p$, displayed in both parameterizations. Left: the prior is flat on $p$ (blue dashed line), and the histogram of 1000 samples confirms this. Right: the same 1000 samples transformed to the log-odds scale $\\eta = \\text{logit}(p)$. The induced density on $\\eta$ (blue dashed curve) is the standard logistic density, which concentrates near $\\eta = 0$ (i.e., $p = 1/2$). A prior that is \"uninformative\" on $p$ is informative on $\\eta$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fig3-flat-eta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Two \"noninformative\" priors — density curves only (no histograms)\n",
    "# The flat-on-η prior is improper, so we show density curves rather than samples.\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "\n",
    "# --- Left panel: p-parameterization ---\n",
    "p_grid = np.linspace(0.005, 0.995, 500)\n",
    "\n",
    "# Flat-on-eta induced density on p: proportional to 1/(p(1-p))\n",
    "# Rescale so the visual area approximately matches the uniform (which integrates to 1)\n",
    "flat_eta_unnorm = 1.0 / (p_grid * (1 - p_grid))\n",
    "area = np.trapz(flat_eta_unnorm, p_grid)\n",
    "flat_eta_on_p = flat_eta_unnorm / area  # now integrates to ~1 over the displayed range\n",
    "\n",
    "ax1.plot(p_grid, flat_eta_on_p, color=COLOR_APPROX, linewidth=2.5,\n",
    "         linestyle='--', label='Flat-on-$\\\\eta$: $\\\\propto 1/(p(1-p))$')\n",
    "ax1.axhline(1.0, color=COLOR_PRIOR, linewidth=2, linestyle='--', alpha=0.7,\n",
    "            label='Uniform-on-$p$: density')\n",
    "ax1.set_xlabel('$p$', fontsize=12)\n",
    "ax1.set_ylabel('Density', fontsize=11)\n",
    "ax1.set_title('Two \"noninformative\" priors\\n($p$-parameterization)',\n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 6)\n",
    "ax1.legend(fontsize=9)\n",
    "\n",
    "# --- Right panel: eta-parameterization ---\n",
    "eta_grid = np.linspace(-8, 8, 500)\n",
    "\n",
    "# Flat-on-eta: constant (improper)\n",
    "ax2.axhline(0.1, color=COLOR_APPROX, linewidth=2.5, linestyle='--',\n",
    "            label='Flat-on-$\\\\eta$: $\\\\propto 1$ (improper)')\n",
    "\n",
    "# Uniform-on-p induced density on eta: logistic density\n",
    "logistic_density = np.exp(eta_grid) / (1 + np.exp(eta_grid))**2\n",
    "ax2.plot(eta_grid, logistic_density, color=COLOR_PRIOR, linewidth=2,\n",
    "         linestyle='--', alpha=0.7, label='Uniform-on-$p$: induced density')\n",
    "\n",
    "ax2.set_xlabel(r'$\\eta = \\log(p/(1-p))$', fontsize=12)\n",
    "ax2.set_ylabel('Density', fontsize=11)\n",
    "ax2.set_title('Two \"noninformative\" priors\\n($\\\\eta$-parameterization)',\n",
    "              fontsize=12, fontweight='bold')\n",
    "ax2.set_xlim(-8, 8)\n",
    "ax2.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fig3-caption",
   "metadata": {},
   "source": [
    "*__Figure 3.__ Two \"noninformative\" priors for the binomial success probability, shown side by side in both parameterizations. Blue (dashed): Uniform on $p$. Red (dashed): flat on the log-odds $\\eta = \\text{logit}(p)$, which is improper (does not integrate to 1). Left panel: in the $p$-parameterization, the flat-on-$\\eta$ prior (red) is proportional to $1/(p(1-p))$ (rescaled so the minimum at $p = 1/2$ equals 1). It is U-shaped, concentrating near $p = 0$ and $p = 1$, and diverges to $\\infty$ at the boundaries. Right panel: in the $\\eta$-parameterization, the flat-on-$\\eta$ prior is constant (red), while the uniform-on-$p$ prior (blue) induces the standard logistic density concentrating near $\\eta = 0$. The two \"noninformative\" priors disagree — \"flat\" depends on the coordinate system.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec3-jeffreys",
   "metadata": {},
   "source": [
    "### The lesson\n",
    "\n",
    "\"Flat\" is not a well-defined concept for continuous parameters — it depends on the coordinate system. These two \"noninformative\" priors give different posteriors, so which one is really noninformative?\n",
    "\n",
    "### The Jeffreys prior: reparameterization invariance\n",
    "\n",
    "**Jeffreys' idea**: Choose the prior $\\pi(\\theta) \\propto \\sqrt{I(\\theta)}$, where $I(\\theta)$ is the Fisher information.\n",
    "\n",
    "**Key property** (stated without proof): The Jeffreys prior is **invariant under reparameterization**. If we change from $\\theta$ to $\\eta = g(\\theta)$, the Jeffreys prior in $\\eta$ is just the change-of-variables transformation of the Jeffreys prior in $\\theta$. It doesn't matter which parameterization we work in — we always get the same answer.\n",
    "\n",
    "### Computing Jeffreys priors: examples\n",
    "\n",
    "**Binomial$(n, p)$.**  The Fisher information is $I(p) = n/(p(1-p))$, so the Jeffreys prior is $\\pi(p) \\propto p^{-1/2}(1-p)^{-1/2}$. This is the $\\text{Beta}(1/2, 1/2)$ distribution — not flat on $p$ or on the log-odds $\\eta$, but something in between.\n",
    "\n",
    "**Exponential$(\\lambda)$.**  The Fisher information is $I(\\lambda) = 1/\\lambda^2$, so the Jeffreys prior is $\\pi(\\lambda) \\propto 1/\\lambda$. This is **improper** — it doesn't integrate to a finite value — but the posterior is proper once we observe data. In the pseudo-data interpretation, the Jeffreys prior can be viewed as the limit of $\\text{Gamma}(\\varepsilon, \\varepsilon \\bar{x})$ as $\\varepsilon \\to 0$ for any pseudo-mean $\\bar{x} > 0$: it corresponds to *zero* pseudo-observations. In this sense, the Jeffreys prior is even weaker than the flat prior $\\pi(\\lambda) \\propto 1$, which corresponds to $\\text{Gamma}(1, 0)$ — one pseudo-observation of $x = 0$. As a result, the Jeffreys posterior concentrates on the MLE, whereas the flat-prior posterior converges to a slightly shrunk estimator (reminiscent of Laplace's estimator from Lecture 6).\n",
    "\n",
    "**Normal$(\\mu, \\sigma^2$ known$)$.**  The Fisher information is $I(\\mu) = 1/\\sigma^2$ (constant), so $\\pi(\\mu) \\propto 1$. Here the Jeffreys prior agrees with the flat prior — because the Normal location family is translation-invariant: shifting $\\mu$ doesn't change the shape of the likelihood, only its location.\n",
    "\n",
    "### Limitations and philosophical tension\n",
    "\n",
    "Jeffreys priors can be improper, and for multiparameter models they can behave poorly. In practice, they are most useful as a principled default for simple models.\n",
    "\n",
    "There's a deeper tension: the subjective posterior had the problem that it was someone's personal opinion. The objective posterior has the opposite problem — it's *nobody's* opinion. So why should we care about it? A practical answer: sometimes we just want to get out of the way of Bernstein–von Mises and let the data speak, without anyone suspecting we cooked up the prior to get the answer we wanted.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec4",
   "metadata": {},
   "source": [
    "## 4. Convenience Priors\n",
    "\n",
    "A **convenience prior** is one that's computationally easy to work with — typically a conjugate prior. This is compatible with multiple philosophies:\n",
    "\n",
    "- **Informative**: Choose a conjugate prior that matches your prior mean and variance (e.g., a seismologist encoding beliefs via a Gamma prior)\n",
    "- **Objective**: Use a flat or Jeffreys prior, if it happens to be conjugate (e.g., flat prior for the Normal mean)\n",
    "- **Weakly informative**: Choose conjugate parameters that gently constrain $\\theta$ without strongly influencing the posterior (e.g., $\\text{Beta}(1,1)$, $\\text{Gamma}(1, \\text{small})$)\n",
    "\n",
    "The key advantage is **computational tractability**: conjugate priors give closed-form posteriors.\n",
    "\n",
    "If we were truly trying to elicit our subjective beliefs, they probably wouldn't correspond to a conjugate family. For example, if I pull a coin out of my pocket, I probably think there's a very high chance it's a normal coin ($p \\approx 0.5$), but a small chance it's a trick coin ($p$ near 0 or 1). That belief — a spike at 0.5 with small bumps near 0 and 1 — is not a Beta density. (See the board sketch in lecture.)\n",
    "\n",
    "But the convenience prior approach is especially appealing when we expect the prior doesn't matter much for the inference — then we might as well pick something easy to compute with.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec5-header",
   "metadata": {},
   "source": [
    "## 5. Hierarchical Bayes\n",
    "\n",
    "### Motivation: many related problems\n",
    "\n",
    "So far, we've been choosing a prior based on philosophy or convenience. But what if we have **many similar problems** whose data can help inform the prior?\n",
    "\n",
    "### Example: baseball batting averages (Efron & Morris, 1975)\n",
    "\n",
    "There are 18 baseball players in our dataset, each with a true batting average $p_i$. For player $i$, we observe $X_i$ hits in $n_i$ at-bats: $X_i \\mid p_i \\sim \\text{Binomial}(n_i, p_i)$.\n",
    "\n",
    "The **naive approach** estimates each player independently by the MLE $\\hat{p}_i = X_i / n_i$. But early in the season, every player has few at-bats, leading to noisy estimates. Roberto Clemente's 18-for-45 (0.400) makes him look like a superstar, while Max Alvis's 7-for-45 (0.156) makes him look terrible — but how much should we trust these numbers?\n",
    "\n",
    "### The hierarchical model\n",
    "\n",
    "Model the $p_i$'s as drawn from a common **population distribution**:\n",
    "\n",
    "- **Hyperparameters**: $\\alpha, \\beta > 0$ (unknown)\n",
    "- **Parameters**: $p_1, \\ldots, p_m \\mid \\alpha, \\beta \\overset{\\text{iid}}{\\sim} \\text{Beta}(\\alpha, \\beta)$\n",
    "- **Data**: $X_i \\mid p_i \\sim \\text{Binomial}(n_i, p_i)$, independently\n",
    "\n",
    "The population distribution $\\text{Beta}(\\alpha, \\beta)$ acts as a **learned prior** — we don't specify $\\alpha$ and $\\beta$ in advance; the data across all players help us learn them.\n",
    "\n",
    "### Shrinkage toward the group mean\n",
    "\n",
    "The posterior for each $p_i$ \"borrows strength\" from all the other players, pulling each estimate toward the group average. This is **shrinkage** — the same idea from Lecture 6 (Laplace's estimator), but now the shrinkage target is learned from the data. With season-ending batting averages as ground truth, we can check whether the shrinkage estimates are actually better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batting-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efron & Morris (1975) dataset: 18 MLB players from the 1970 season\n",
    "# Each player had exactly 45 at-bats early in the season\n",
    "# Season-ending batting averages provide ground truth\n",
    "\n",
    "batting = pd.DataFrame({\n",
    "    'player': [\n",
    "        'Clemente',    'F Robinson', 'Howard',     'Johnstone',\n",
    "        'Berry',       'Spencer',    'Kessinger',  'Alvarado',\n",
    "        'Santo',       'Swoboda',    'Petrocelli', 'Rodriguez',\n",
    "        'Scott',       'Unser',      'Williams',   'Campaneris',\n",
    "        'Munson',      'Alvis',\n",
    "    ],\n",
    "    'hits': [\n",
    "        18, 17, 16, 15,\n",
    "        14, 14, 13, 12,\n",
    "        11, 11, 10, 10,\n",
    "        10, 10, 10,  9,\n",
    "         8,  7,\n",
    "    ],\n",
    "    'at_bats': [45] * 18,\n",
    "    'season_ba': [\n",
    "        .352, .306, .283, .238,\n",
    "        .276, .274, .266, .224,\n",
    "        .267, .233, .261, .225,\n",
    "        .296, .258, .251, .279,\n",
    "        .302, .183,\n",
    "    ],\n",
    "})\n",
    "\n",
    "batting['mle'] = batting['hits'] / batting['at_bats']\n",
    "m = len(batting)\n",
    "\n",
    "print(f\"Number of players: {m}\")\n",
    "print(f\"At-bats per player: {batting['at_bats'].iloc[0]}\")\n",
    "print(f\"MLE range: {batting['mle'].min():.3f} to {batting['mle'].max():.3f}\")\n",
    "print(f\"Overall batting average: {batting['hits'].sum() / batting['at_bats'].sum():.3f}\")\n",
    "print()\n",
    "print(batting[['player', 'hits', 'at_bats', 'mle', 'season_ba']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec5-data-desc",
   "metadata": {},
   "source": [
    "This is the classic dataset from Efron & Morris (1975): 18 MLB players from the 1970 season, each with exactly $n = 45$ at-bats early in the season. The MLEs (hits/45) range from Clemente's 0.400 to Alvis's 0.156. The `season_ba` column records each player's batting average at the end of the full season — this serves as approximate \"ground truth\" for evaluating our estimates.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec6-header",
   "metadata": {},
   "source": [
    "## 6. The Gibbs Sampler\n",
    "\n",
    "### Why we need computation\n",
    "\n",
    "In the hierarchical model, the posterior $\\pi(p_1, \\ldots, p_m, \\alpha, \\beta \\mid X_1, \\ldots, X_m)$ has no closed-form expression. We can't just recognize the functional form as we did with conjugate models. We need a computational method to **sample** from this posterior.\n",
    "\n",
    "### The Gibbs sampler\n",
    "\n",
    "Instead of sampling all parameters at once, we cycle through them one at a time, sampling each from its **full conditional distribution** — the distribution of that parameter given all the others and the data.\n",
    "\n",
    "For our hierarchical model, we reparameterize from $(\\alpha, \\beta)$ to $\\mu = \\alpha/(\\alpha + \\beta)$ (population mean) and $\\sigma^2 = \\mu(1-\\mu)/(\\alpha + \\beta + 1)$ (population variance), with a flat hyperprior on $(\\mu, \\sigma^2)$. This is more interpretable and reduces correlation between the hyperparameters. In each iteration:\n",
    "\n",
    "- **Step (a)**: Sample each $p_i$ from its conjugate Beta posterior (given $\\alpha = \\alpha(\\mu, \\sigma^2)$ and $\\beta = \\beta(\\mu, \\sigma^2)$) — easy!\n",
    "- **Step (b)**: Sample $\\mu$ from $\\pi(\\mu \\mid \\sigma^2, p_1, \\ldots, p_m)$ — one-dimensional, computed via grid approximation\n",
    "- **Step (c)**: Sample $\\sigma^2$ from $\\pi(\\sigma^2 \\mid \\mu, p_1, \\ldots, p_m)$ — same\n",
    "\n",
    "The $p_i$ updates are fast (closed-form Beta draws). The $\\mu$ and $\\sigma^2$ updates require grid approximation, but each is just a one-dimensional problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gibbs-helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_sigma2_to_alpha_beta(mu, sigma2):\n",
    "    \"\"\"Convert (mu, sigma2) parameterization to (alpha, beta).\"\"\"\n",
    "    kappa = mu * (1 - mu) / sigma2 - 1  # concentration = alpha + beta\n",
    "    return mu * kappa, (1 - mu) * kappa\n",
    "\n",
    "def log_conditional_mu(mu, sigma2, p_vec):\n",
    "    \"\"\"Log full conditional for mu (population mean) given sigma2 and p_1,...,p_m.\"\"\"\n",
    "    m = len(p_vec)\n",
    "    kappa = mu * (1 - mu) / sigma2 - 1\n",
    "    if kappa <= 0:\n",
    "        return -np.inf\n",
    "    alpha = mu * kappa\n",
    "    beta = (1 - mu) * kappa\n",
    "    return (alpha - 1) * np.sum(np.log(p_vec)) + (beta - 1) * np.sum(np.log(1 - p_vec)) - m * betaln(alpha, beta)\n",
    "\n",
    "def log_conditional_sigma2(sigma2, mu, p_vec):\n",
    "    \"\"\"Log full conditional for sigma2 (population variance) given mu and p_1,...,p_m.\"\"\"\n",
    "    m = len(p_vec)\n",
    "    kappa = mu * (1 - mu) / sigma2 - 1\n",
    "    if kappa <= 0:\n",
    "        return -np.inf\n",
    "    alpha = mu * kappa\n",
    "    beta = (1 - mu) * kappa\n",
    "    return (alpha - 1) * np.sum(np.log(p_vec)) + (beta - 1) * np.sum(np.log(1 - p_vec)) - m * betaln(alpha, beta)\n",
    "\n",
    "def sample_from_log_grid(log_func, grid, *args):\n",
    "    \"\"\"Sample from a 1D distribution via grid approximation.\n",
    "    \n",
    "    Evaluates log_func on the grid, exponentiates with numerical stability,\n",
    "    normalizes to a PMF, and draws one sample.\n",
    "    \"\"\"\n",
    "    log_vals = np.array([log_func(g, *args) for g in grid])\n",
    "    log_vals -= np.max(log_vals)  # numerical stability\n",
    "    probs = np.exp(log_vals)\n",
    "    probs /= probs.sum()\n",
    "    return np.random.choice(grid, p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gibbs-sampler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gibbs sampler for the beta-binomial hierarchical model ---\n",
    "# Parameterized by mu (population mean) and sigma2 (population variance),\n",
    "# with flat hyperpriors on (mu, sigma2). More interpretable than (alpha, beta).\n",
    "\n",
    "n_iter = 5000\n",
    "burn_in = 1000\n",
    "\n",
    "# Grids for mu and sigma2\n",
    "mu_grid = np.linspace(0.01, 0.99, 300)\n",
    "sigma2_grid = np.linspace(0.00005, 0.01, 300)\n",
    "\n",
    "# Data\n",
    "hits = batting['hits'].values\n",
    "n_trials = batting['at_bats'].values\n",
    "\n",
    "# Initialize\n",
    "mu_current = 0.265\n",
    "sigma2_current = 0.005\n",
    "p_current = batting['mle'].values.copy()\n",
    "p_current = np.clip(p_current, 0.01, 0.99)\n",
    "\n",
    "# Storage\n",
    "mu_trace = np.zeros(n_iter)\n",
    "sigma2_trace = np.zeros(n_iter)\n",
    "p_traces = np.zeros((n_iter, m))\n",
    "\n",
    "for t in range(n_iter):\n",
    "    # Step (a): Sample each p_i from Beta(X_i + alpha, n_i - X_i + beta)\n",
    "    alpha_current, beta_current = mu_sigma2_to_alpha_beta(mu_current, sigma2_current)\n",
    "    for i in range(m):\n",
    "        a_post = hits[i] + alpha_current\n",
    "        b_post = n_trials[i] - hits[i] + beta_current\n",
    "        p_current[i] = np.random.beta(a_post, b_post)\n",
    "    \n",
    "    # Step (b): Sample mu via grid approximation\n",
    "    mu_current = sample_from_log_grid(\n",
    "        log_conditional_mu, mu_grid, sigma2_current, p_current)\n",
    "    \n",
    "    # Step (c): Sample sigma2 via grid approximation\n",
    "    sigma2_current = sample_from_log_grid(\n",
    "        log_conditional_sigma2, sigma2_grid, mu_current, p_current)\n",
    "    \n",
    "    mu_trace[t] = mu_current\n",
    "    sigma2_trace[t] = sigma2_current\n",
    "    p_traces[t, :] = p_current\n",
    "\n",
    "print(f\"Gibbs sampler complete: {n_iter} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fig4-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4: Trace plots for mu (population mean) and sigma2 (population variance)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
    "\n",
    "ax1.plot(mu_trace, color=MULTI_BLUES[0], linewidth=0.5, alpha=0.7)\n",
    "ax1.axvline(burn_in, color=COLOR_APPROX, linestyle='--', linewidth=1.5,\n",
    "            label=f'Burn-in ({burn_in} iter)')\n",
    "ax1.set_ylabel(r'$\\mu$ (population mean)', fontsize=13)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.set_title('Gibbs Sampler Trace Plots', fontsize=13, fontweight='bold')\n",
    "\n",
    "ax2.plot(sigma2_trace, color=MULTI_BLUES[1], linewidth=0.5, alpha=0.7)\n",
    "ax2.axvline(burn_in, color=COLOR_APPROX, linestyle='--', linewidth=1.5)\n",
    "ax2.set_ylabel(r'$\\sigma^2$ (population variance)', fontsize=13)\n",
    "ax2.set_xlabel('Iteration', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fig4-caption",
   "metadata": {},
   "source": [
    "*__Figure 4.__ Trace plots for the hyperparameters $\\mu$ (population mean, top) and $\\sigma^2$ (population variance, bottom) from the Gibbs sampler applied to the beta-binomial hierarchical model. The red dashed line marks the end of the burn-in period (1000 iterations). After burn-in, both chains appear stationary, indicating convergence to the posterior distribution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posterior-summaries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior summaries from post-burn-in samples\n",
    "mu_post = mu_trace[burn_in:]\n",
    "sigma2_post = sigma2_trace[burn_in:]\n",
    "p_post = p_traces[burn_in:, :]\n",
    "\n",
    "posterior_means = p_post.mean(axis=0)\n",
    "group_mean = mu_post.mean()\n",
    "\n",
    "print(f\"Posterior mean of mu (group mean):     {mu_post.mean():.3f}\")\n",
    "print(f\"Posterior mean of sigma2 (pop. var.):  {sigma2_post.mean():.5f}\")\n",
    "print(f\"Posterior SD of batting averages:      {np.sqrt(sigma2_post.mean()):.3f}\")\n",
    "print()\n",
    "print(f\"{'Player':<14s} {'MLE':<8s} {'Post. mean':<12s} {'Season BA':<10s} {'Shrinkage'}\")\n",
    "print(\"-\" * 58)\n",
    "for i in range(m):\n",
    "    shrink = batting['mle'].iloc[i] - posterior_means[i]\n",
    "    print(f\"{batting['player'].iloc[i]:<14s} \"\n",
    "          f\"{batting['mle'].iloc[i]:<8.3f} {posterior_means[i]:<12.3f} \"\n",
    "          f\"{batting['season_ba'].iloc[i]:<10.3f} {shrink:+.3f}\")\n",
    "\n",
    "# MSE comparison: MLE vs season BA, posterior mean vs season BA\n",
    "mse_mle = np.mean((batting['mle'].values - batting['season_ba'].values)**2)\n",
    "mse_post = np.mean((posterior_means - batting['season_ba'].values)**2)\n",
    "print(f\"\\nMSE comparison (predicting season-ending BA):\")\n",
    "print(f\"  MLE:            {mse_mle:.5f}\")\n",
    "print(f\"  Posterior mean:  {mse_post:.5f}\")\n",
    "print(f\"  Improvement:     {(1 - mse_post/mse_mle)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fig5-shrinkage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 5: Shrinkage plot — MLE vs posterior mean, with season-ending BA as ground truth\n",
    "# Red circles + lines show MLE errors (shifted left); blue circles + lines show posterior mean errors (shifted right)\n",
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "# Determine plot range\n",
    "all_vals = np.concatenate([batting['mle'].values, posterior_means, batting['season_ba'].values])\n",
    "lo = max(0, min(all_vals) - 0.03)\n",
    "hi = min(1, max(all_vals) + 0.03)\n",
    "\n",
    "offset = 0.005  # horizontal shift so red and blue lines don't overlap\n",
    "\n",
    "# 45-degree line (no shrinkage)\n",
    "ax.plot([lo, hi], [lo, hi], color=COLOR_LIKELIHOOD, linestyle='--',\n",
    "        linewidth=1.5, alpha=0.5, label='No shrinkage ($y = x$)')\n",
    "\n",
    "# Group mean line\n",
    "ax.axhline(group_mean, color=COLOR_POSTERIOR, linestyle='--', linewidth=1.5,\n",
    "           alpha=0.5, label=f'Group mean ({group_mean:.3f})')\n",
    "\n",
    "for i in range(m):\n",
    "    mle_i = batting['mle'].iloc[i]\n",
    "    pm_i = posterior_means[i]\n",
    "    sba_i = batting['season_ba'].iloc[i]\n",
    "    \n",
    "    # Red line + circle (shifted left): MLE to season BA\n",
    "    ax.plot([mle_i - offset, mle_i - offset], [mle_i, sba_i],\n",
    "            color=COLOR_APPROX, linewidth=1, alpha=0.5)\n",
    "    ax.scatter(mle_i - offset, mle_i, c=COLOR_APPROX, s=50,\n",
    "               edgecolors='white', linewidths=0.5, zorder=4)\n",
    "    \n",
    "    # Blue line + circle (shifted right): posterior mean to season BA\n",
    "    ax.plot([mle_i + offset, mle_i + offset], [pm_i, sba_i],\n",
    "            color=COLOR_POSTERIOR, linewidth=1, alpha=0.5)\n",
    "    ax.scatter(mle_i + offset, pm_i, c=COLOR_POSTERIOR, s=70,\n",
    "               edgecolors='white', linewidths=0.5, zorder=5)\n",
    "    \n",
    "    # Black x: season-ending BA (centered)\n",
    "    ax.scatter(mle_i, sba_i, c='black', s=50, marker='x', linewidths=1.5, zorder=6)\n",
    "\n",
    "# --- Non-overlapping player labels ---\n",
    "# Collect (name, point_x, point_y) sorted by posterior mean\n",
    "label_info = [(batting['player'].iloc[i], batting['mle'].iloc[i] + offset, posterior_means[i])\n",
    "              for i in range(m)]\n",
    "label_info.sort(key=lambda t: t[2])\n",
    "\n",
    "# Compute adjusted y-positions with minimum vertical separation\n",
    "min_sep = 0.013\n",
    "adj_ys = [label_info[0][2]]\n",
    "for j in range(1, len(label_info)):\n",
    "    y = max(label_info[j][2], adj_ys[-1] + min_sep)\n",
    "    adj_ys.append(y)\n",
    "\n",
    "# Center the label block to minimize overall drift\n",
    "drift = np.mean(adj_ys) - np.mean([t[2] for t in label_info])\n",
    "adj_ys = [y - drift for y in adj_ys]\n",
    "\n",
    "# Re-enforce minimum separation after centering\n",
    "for j in range(1, len(adj_ys)):\n",
    "    if adj_ys[j] - adj_ys[j-1] < min_sep:\n",
    "        adj_ys[j] = adj_ys[j-1] + min_sep\n",
    "\n",
    "# Place labels in a right-side column with connecting lines\n",
    "label_x = hi - 0.005\n",
    "for j, (name, px, py) in enumerate(label_info):\n",
    "    ax.annotate(name, xy=(px, py), xytext=(label_x, adj_ys[j]),\n",
    "                fontsize=7.5, color=COLOR_POSTERIOR, va='center', ha='right',\n",
    "                arrowprops=dict(arrowstyle='-', color=COLOR_POSTERIOR, alpha=0.25, lw=0.5))\n",
    "\n",
    "# Legend entries for marker types\n",
    "ax.scatter([], [], c=COLOR_APPROX, s=50, edgecolors='white', label='MLE (early season)')\n",
    "ax.plot([], [], color=COLOR_APPROX, linewidth=1, alpha=0.5, label='MLE error')\n",
    "ax.scatter([], [], c=COLOR_POSTERIOR, s=70, edgecolors='white', label='Posterior mean')\n",
    "ax.plot([], [], color=COLOR_POSTERIOR, linewidth=1, alpha=0.5, label='Posterior mean error')\n",
    "ax.scatter([], [], c='black', s=50, marker='x', linewidths=1.5, label='Season-ending BA (ground truth)')\n",
    "\n",
    "ax.set_xlabel('MLE ($\\\\hat{p}_i = X_i / 45$)', fontsize=12)\n",
    "ax.set_ylabel('Estimate / Ground Truth', fontsize=12)\n",
    "ax.set_title('Shrinkage: MLEs vs Posterior Means\\n(Efron & Morris, 1975)',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.set_xlim(lo, hi)\n",
    "ax.set_ylim(lo, hi)\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(fontsize=9, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fig5-caption",
   "metadata": {},
   "source": [
    "*__Figure 5.__ Shrinkage in the beta-binomial hierarchical model applied to the Efron & Morris (1975) data. Each player's x-coordinate is the early-season MLE (hits/45). Red circles sit on the $y = x$ line (i.e., the MLE \"prediction\"); blue circles show posterior means; black × markers show season-ending batting averages (ground truth). Red lines connect each MLE to the ground truth, and blue lines connect each posterior mean to the ground truth — shorter lines mean smaller prediction errors. For most players the blue lines are shorter than the red lines, confirming that shrinkage toward the group mean (blue dashed) produces better predictions of season-ending performance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec6-discussion",
   "metadata": {},
   "source": [
    "### Why shrinkage makes sense\n",
    "\n",
    "All 18 players have the same number of at-bats ($n = 45$), so each gets the same degree of shrinkage toward the group mean. The key insight is that early-season batting averages are noisy — 45 at-bats is not a lot — and shrinkage toward the group average produces better predictions of season-ending performance. The MSE comparison above confirms this: the posterior means are substantially closer to the season-ending batting averages than the raw MLEs.\n",
    "\n",
    "This is the same idea as Lecture 6: Laplace's estimator $(X + k)/(n + 2k)$ shrinks toward 1/2 (an arbitrary, fixed target). The hierarchical model does the same thing, but the shrinkage target is **learned from the data** — it's the overall group batting average, not a predetermined value.\n",
    "\n",
    "### Why the Gibbs sampler works\n",
    "\n",
    "Each conditional update **preserves** the target distribution $\\pi(p_1, \\ldots, p_m, \\alpha, \\beta \\mid X)$. If the current state is a draw from the joint posterior and we resample one coordinate from its conditional, the result is still a draw from the joint posterior. Since each step preserves the target, the full cycle does too — so the posterior is a **stationary distribution** of the Markov chain. The chain also needs to be irreducible and aperiodic to guarantee convergence, but these hold under mild conditions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### Four approaches to priors\n",
    "\n",
    "| Approach | Key Idea | Pros | Cons |\n",
    "|----------|----------|------|------|\n",
    "| **Subjective Bayes** | Prior encodes personal degrees of belief | Can incorporate expertise | Whose beliefs? Hard to justify scientifically |\n",
    "| **Objective Bayes** | Flat or Jeffreys prior; minimize prior influence | Observer-agnostic | \"Nobody's opinion\"; Jeffreys can be improper |\n",
    "| **Convenience priors** | Conjugate priors for tractability | Closed-form posteriors | May not match actual beliefs |\n",
    "| **Hierarchical Bayes** | Learn the prior from data across related problems | Shrinkage; borrows strength | Computationally harder; needs many related problems |\n",
    "\n",
    "### Key technical results\n",
    "\n",
    "- **Jeffreys prior**: $\\pi(\\theta) \\propto \\sqrt{I(\\theta)}$. Computed for Binomial $\\to$ $\\text{Beta}(1/2, 1/2)$; Exponential $\\to$ $\\pi(\\lambda) \\propto 1/\\lambda$; Normal location $\\to$ flat\n",
    "- **Hierarchical Bayes**: The population distribution serves as a learned prior; the Gibbs sampler provides a computational tool for posterior inference\n",
    "- **Gibbs sampler**: Cycle through parameters, sampling each from its full conditional. Preserves the target distribution because resampling one coordinate from its conditional doesn't change the joint.\n",
    "\n",
    "### Key takeaways\n",
    "\n",
    "1. Priors are uncheckable — unlike the model, there's no diagnostic to tell you your prior is wrong\n",
    "2. Flat priors are a simple noninformative default, but they're not reparameterization-invariant; the Jeffreys prior $\\pi(\\theta) \\propto \\sqrt{I(\\theta)}$ resolves this ambiguity\n",
    "3. Hierarchical Bayes lets the data inform the prior across many similar problems, producing shrinkage estimators that borrow strength across units\n",
    "4. The Gibbs sampler is a practical MCMC algorithm for posterior computation: cycle through coordinates, sample each from its full conditional\n",
    "\n",
    "**Next time (Lecture 9):** What if the model itself is wrong? Model misspecification and Kullback-Leibler divergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data145",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
