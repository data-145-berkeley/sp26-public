{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Lecture 5: Asymptotic Normality and Efficiency of the MLE\n",
    "\n",
    "**Data 145, Spring 2026: Evidence and Uncertainty**  \n",
    "**Instructors:** Ani Adhikari, William Fithian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023b106-9cca-45fe-a354-11ee94b36f06",
   "metadata": {},
   "source": [
    "---\n",
    "As in lectures 3 and 4, **our focus is on single-parameter models** where the distribution is known up to one real number $\\theta$. We are using the method of maximum likelihood to estimate $\\hat{\\theta}$.\n",
    "\n",
    "## Road Map for the Lecture\n",
    "\n",
    "1. Quickly recap some terminology and notation.\n",
    "2. Derive an alternative way to calculate the Fisher information. (Provided as part of Lecture 4 notes)\n",
    "3. Derive (apart from some technical issues) the asymptotic normality of the MLE and identify its variance. (Provided as Part of Lecture 4 notes)\n",
    "4. Define the relative efficiency of two estimators.\n",
    "5. Examine the efficiency of the MLE by applying the Cramér-Rao theorem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfae735f-8b2c-4fd8-aea7-d565f49ac102",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Recap: Terminology and Notation\n",
    "\n",
    "\n",
    "Given i.i.d. data $X_1, \\ldots, X_n$ from density $f_\\theta$, the **likelihood function** is:\n",
    "\n",
    "$$\\text{Lik}(\\theta; X) = \\prod_{i=1}^n f_\\theta(X_i)$$\n",
    "\n",
    "This is the joint density of the data, viewed as a function of $\\theta$ (with data held fixed).\n",
    "\n",
    "The **log-likelihood** is:\n",
    "\n",
    "$$\\ell_n(\\theta; X) = \\log \\text{Lik}(\\theta; X) = \\sum_{i=1}^n \\log f_\\theta(X_i)$$\n",
    "\n",
    "The subscript $n$ reminds us that $X$ is a sample of size $n$.\n",
    "\n",
    "The **maximum likelihood estimator** (MLE) is:\n",
    "\n",
    "$$\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_{\\theta \\in \\Theta} \\text{Lik}(\\theta; X) = \\arg\\max_{\\theta \\in \\Theta} \\ell_n(\\theta; X)$$\n",
    "\n",
    "The **score function** (or just **score**) is the derivative of the log-likelihood with respect to $\\theta$:\n",
    "\n",
    "$$S_n(\\theta; X) = \\ell_n'(\\theta; X) = \\frac{\\partial}{\\partial \\theta} \\ell_n(\\theta; X)$$\n",
    "\n",
    "The **Fisher information** in a single observation $X_i$ is defined by:\n",
    "\n",
    "$$I(\\theta) = Var_\\theta(\\ell_1'(\\theta; X_i)) = E_\\theta(\\ell_1'(\\theta; X_i)^2)$$\n",
    "\n",
    "The second equality uses $E_\\theta(\\ell_1'(\\theta; X_i)) = 0$.\n",
    "\n",
    "For $n$ i.i.d. observations, the **total Fisher information** is $nI(\\theta)$, since:\n",
    "$$Var_\\theta(S_n(\\theta; X)) = n \\cdot Var_\\theta(\\ell_1'(\\theta; X_i)) = nI(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fisher-second-deriv",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. An Alternative Formula for the Fisher Information\n",
    "\n",
    "There's another way to compute Fisher information that's often more convenient for calculations. \n",
    "\n",
    "Assume that $\\ell$ is twice differentiable.\n",
    "\n",
    "**Theorem:** $I(\\theta) = -E_\\theta[\\ell_1''(\\theta; X_i)]$\n",
    "\n",
    "**Proof:** We showed that $E_\\theta[\\ell_1'(\\theta; X_i)] = 0$ for all $\\theta$. Differentiate both sides with respect to $\\theta$:\n",
    "\n",
    "$$0 = \\frac{d}{d\\theta} E_\\theta[\\ell_1'(\\theta; X_i)] = \\frac{d}{d\\theta} \\int \\ell_1'(\\theta; x) f_\\theta(x) \\, dx$$\n",
    "\n",
    "Switch the derivative and integral, and use the product rule of derivatives:\n",
    "$$0 = \\int \\ell_1''(\\theta; x) \\cdot f_\\theta(x) \\, dx + \\int \\ell_1'(\\theta; x) \\cdot \\frac{d}{d\\theta} f_\\theta(x) \\, dx$$\n",
    "\n",
    "The first integral is $E_\\theta[\\ell_1''(\\theta; X_i)]$. For the second, note that $\\frac{d}{d\\theta} f_\\theta(x) = \\ell_1'(\\theta; x) \\cdot f_\\theta(x)$, so:\n",
    "$$\\int \\ell_1'(\\theta; x) \\cdot \\frac{d}{d\\theta} f_\\theta(x) \\, dx = \\int \\ell_1'(\\theta; x)^2 f_\\theta(x) \\, dx = E_\\theta[\\ell_1'(\\theta; X_i)^2] = I(\\theta)$$\n",
    "\n",
    "Therefore: $0 = E_\\theta[\\ell_1''(\\theta; X_i)] + I(\\theta)$, giving $I(\\theta) = -E_\\theta[\\ell_1''(\\theta; X_i)]$. $\\square$\n",
    "\n",
    "**Interpretation:** The Fisher information equals the negative expected curvature of the log-likelihood. More curvature at the maximum means the MLE is more precisely determined.\n",
    "\n",
    "#### Check the New Formula in the Normal Case\n",
    "If the sample is i.i.d. normal $(\\mu, \\sigma^2)$ for a known $\\sigma^2$, we have seen that\n",
    "$$\n",
    "\\ell_1'(\\mu; X_i) = \\frac{X_i - \\mu}{\\sigma^2}\n",
    "$$\n",
    "so\n",
    "$$\n",
    "\\ell_1''(\\mu; X_i) = -\\frac{1}{\\sigma^2}\n",
    "$$\n",
    "Note that **this is a constant** so its expectation is just itself, and it agrees with $Var_\\mu(\\ell_1'(\\mu; X_i))$ calculated earlier.\n",
    "\n",
    "#### Check the New Formula in the Exponential Case\n",
    "If the sample is i.i.d. exponential with rate $\\lambda$, we have seen that\n",
    "$$\n",
    "\\ell_1'(\\lambda; X_i) = \\frac{1}{\\lambda} - X_i\n",
    "$$\n",
    "so\n",
    "$$\n",
    "\\ell_1''(\\lambda; X_i) = -\\frac{1}{\\lambda^2}\n",
    "$$\n",
    "Once again, it's a constant, so its expectation is just itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a2f89f-0304-4b0f-8b37-a63499fc4116",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### 3.1. Towards Asymptotic Normality: Consistency\n",
    "\n",
    "This was shown in Lecture 3 (apart from some care required to establish uniform convergence instead of pointwise convergence; but don't worry about that).\n",
    "\n",
    "Let $\\hat{\\theta}_n$ be the MLE of the true parameter $\\theta_0$ based on $X_1, X_2, \\ldots, X_n$. \n",
    "\n",
    "Then $\\hat{\\theta}_n$ is a consistent estimator of $\\theta_0$. That is, $\\hat{\\theta}_n \\stackrel{P}{\\to} \\theta_0$.\n",
    "\n",
    "Here the $P$ in the $\\stackrel{P}{\\to}$ symbol is the true underlying probability distribution, that is, $P_{\\theta_0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d6d371-6c6d-40ec-8000-71b727facf8d",
   "metadata": {},
   "source": [
    "### 3.2. Towards Asymptotic Normality: Taylor Expansion\n",
    "\n",
    "First, the statement of asymptotic normality of the MLE:\n",
    "\n",
    "**Theorem:** Let $\\theta_0$ be the true value of $\\theta$. Under regularity conditions that we have assumed without stating, the MLE $\\hat{\\theta}_n$ is asymptotically normal:\n",
    "\n",
    "$$\n",
    "\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\stackrel{d}{\\to} N\\left(0, \\displaystyle{\\frac{1}{I(\\theta_0)}}\\right) ~~~ \\text{ as } n \\to \\infty\n",
    "$$\n",
    "\n",
    "**Organizing a derivation:**\n",
    "The MLE is obtained by setting the derivative of the log-likelihood to be 0. Since the derivative of the log-likelihood is the score function, we have $0 = S_n(\\hat{\\theta}_n; X)$.\n",
    "\n",
    "Since $\\hat{\\theta}_n$ and the true $\\theta_0$ are likely to be close for large $n$, use a Taylor expansion of $S_n\\hat{\\theta}_n; X)$ about $\\theta_0$. For ease of notation, we will suppress the sample $X$ from now on. But it's there, and it's the reason the equalities below are equalities of random variables.\n",
    "\n",
    "$$\n",
    "0 ~ = ~ S_n(\\hat{\\theta}_n) ~=~ S_n(\\theta_0) + (\\hat{\\theta}_n - \\theta_0)S_n'(\\tilde{\\theta}_n)\n",
    "$$\n",
    "for some point $\\tilde{\\theta}_n$ between $\\hat{\\theta}_n$ and $\\theta_0$.\n",
    "\n",
    "Note that we're assuming $S_n$ is differentiable. \n",
    "\n",
    "Rewrite the above to see that\n",
    "$$\\hat{\\theta}_n - \\theta_0 ~ = ~ \\frac{S_n(\\theta_0)}{-S_n'(\\tilde{\\theta}_n)}$$\n",
    "and hence\n",
    "$$\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) ~ = ~ \\frac{\\frac{1}{\\sqrt{n}}S_n(\\theta_0)}\n",
    "{-\\frac{1}{n}S_n'(\\tilde{\\theta}_n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c0c78-3f65-4e62-a3c6-8f55a90fde84",
   "metadata": {},
   "source": [
    "### 3.3. Towards Asymptotic Normality: The Numerator\n",
    "\n",
    "$$S_n(\\theta_0) = \\sum_{i=1}^n \\ell_1'(\\theta_0; X_i)$$\n",
    "\n",
    "This is a sum of i.i.d. random variables with common mean $0$ and common variance $I(\\theta_0)$. Here the mean and variance are calculated using the true $\\theta_0$ as the parameter.\n",
    "\n",
    "By the CLT, \n",
    "$$\\frac{S_n(\\theta_0)}{\\sqrt{nI(\\theta_0)}} \\stackrel{d}{\\to} N(0, 1)$$\n",
    "and hence\n",
    "$$\\frac{S_n(\\theta_0)}{\\sqrt{n}} \\stackrel{d}{\\to} N(0, I(\\theta_0))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db5859-084e-4bd7-b7dc-b65e1f524eea",
   "metadata": {},
   "source": [
    "### 3.4. Towards Asymptotic Normality: The Denominator\n",
    "\n",
    "First note that for any $\\theta$,\n",
    "$$\\frac{1}{n} S_n'(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\ell_1^\"(\\theta)$$\n",
    "\n",
    "This is the mean of an i.i.d. sample. By the Weak Law of Large Numbers,\n",
    "$$ \\frac{1}{n} S_n'(\\theta) \\stackrel{P}{\\to} E_{\\theta}(\\ell_1^\"(\\theta)) = -I(\\theta)$$\n",
    "\n",
    "By consistency of the MLE, $\\hat{\\theta}_n \\stackrel{P}{\\to} \\theta_0$, where the probabilities are calculated using the true $\\theta_0$.\n",
    "\n",
    "By the same \"squeezing\" argument as in the derivation of the delta method, $\\vert \\tilde{\\theta}_n - \\theta_0 \\vert \\le \\vert \\hat{\\theta}_n - \\theta_0 \\vert$ and so $\\tilde{\\theta}_n \\stackrel{P}{\\to} \\theta_0$.\n",
    "\n",
    "We want to conclude that $\\displaystyle \\frac{1}{n} S_n'(\\tilde{\\theta}_n) \\stackrel{P}{\\to} -I(\\theta_0)$ when the probabilities are calculated using the true $\\theta_0$. But we don't quite have that, and it takes some work and regularity conditions to prove.\n",
    "\n",
    "It's fine to simply assume that we have enough regularity to make it work, and therefore the denominator converges in probability to the constant $-I(\\theta_0)$.\n",
    "\n",
    "In fact, in all our examples we've seen that $\\ell_1''(\\theta; X_i)$ is a constant (that is, a non-random quantity) involving $\\theta$. Thich implies $\\displaystyle \\frac{1}{n} S_n'(\\tilde{\\theta}_n)$ is that same quantity for every $n$. So the \"convergence\", if we still want to call it that, is automatically to that quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469990c8-4c6c-4f32-aa05-35a8147a3934",
   "metadata": {},
   "source": [
    "### 3.5. Asymptotic Normality\n",
    "\n",
    "Now use Slutsky's theorem to see that $\\sqrt{n}(\\hat{\\theta}_n - \\theta_0)$ converges in distribution to a normal $(0, I(\\theta_0)$ random variable times the constant $-1/I(\\theta_0)$. That constant is squared in the calculation of the variance, so \n",
    "\n",
    "$$\n",
    "\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\stackrel{d}{\\to} N\\left(0, \\displaystyle{\\frac{1}{I(\\theta_0)}}\\right)\n",
    "$$\n",
    "\n",
    "as we had claimed.\n",
    "\n",
    "From a practical perspective, the result says that for large $n$, the distribution of $\\hat{\\theta}_n$ is approximately normal with mean $\\theta_0$ and variance $1/nI(\\theta_0)$.\n",
    "\n",
    "So if the sample is large, the MLE is almost unbiased, it has a variance we can estimate, and its distribution is approximately normal This allows us to construct confidence intervals for $\\theta_0$, as you have seen in exercises.\n",
    "\n",
    "That's a great property of the MLE. But could we do better? Let's look at a way to compare to estimators of the same parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a0814-03cd-4266-a495-c837b3034846",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Relative Efficiency, and Efficiency\n",
    "\n",
    "One measure of how accurate an estimate is relative to another is *efficiency*. \n",
    "\n",
    "Suppose we are estimating a parameter $\\theta$. The **relative efficiency** of two unbiased estimators $T_1$ and $T_2$ is defined as $\\displaystyle \\frac{Var_\\theta(T_1)}{Var_\\theta(T_2)}$. \n",
    "\n",
    "$T_1$ is considered a better estimator than $T_2$ if this ratio is less than $1$.\n",
    "\n",
    "It's important that we have assumed both estimators to be unbiased. If they had different biases, then we'd have to deal with that before comparing variances.\n",
    "\n",
    "Let $T$ be an unbiased estimator of $\\theta$ based on an i.i.d. sample $X_1, X_2, \\ldots, X_n$. We will define the **efficiency** of $T$ as $\\displaystyle \\frac{1/nI(\\theta)}{Var_\\theta(T)}$, and we will call $T$ **efficient** if its efficiency is $1$.\n",
    "\n",
    "The Cramér-Rao theorem will help us see what this has to do with the MLE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb44189-3d40-4f54-a096-2fa4d71e8dda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Efficiency and the Cramér-Rao Bound\n",
    "\n",
    "#### Cramér-Rao Theorem\n",
    "For any unbiased estimator $T$ of $\\theta$ based on an i.i.d. sample of size $n$, \n",
    "$$Var_{\\theta}(T) ~ \\geq ~ \\frac{1}{nI(\\theta)}$$\n",
    "\n",
    "This allows us to say that MLE $\\hat{\\theta}_n$ is asymptotically efficient. We know that for large $n$ it is almost unbiased and its variance is close to this lower bound. In other words, if we can calculate an MLE, it will be pretty much the best estimator we could get.\n",
    "\n",
    "But keep in mind that the Cramér-Rao theorem is not an asymptotic result. It is true for all $n$.\n",
    "\n",
    "**Proof:** \n",
    "For the score function $S_n(\\theta; X)$, we know that $E_\\theta(S_n(\\theta; X)) = 0$. We also know that $Var(S_n(\\theta; X)) = nI(\\theta)$.\n",
    "\n",
    "Let $\\rho(T, S_n)$ be the correlation between $T$ and $S_n(\\theta; X)$. Then $\\rho(T, S_n) \\le 1$. Thus (skipping the subscript $\\theta$ in all expectations and variances), we have\n",
    "\n",
    "$$\n",
    "\\frac{Cov(T, S_n)}{SD(T)SD(S_n)} ≤ 1\n",
    "$$\n",
    "which is equivalent to\n",
    "$$\n",
    "Var(T) ≥ \\frac{Cov^2(T, S_n)}{Var(S_n)}\n",
    "$$\n",
    "\n",
    "So\n",
    "$$\n",
    "Var(T) ≥ \\frac{Cov^2(T, S_n)}{nI(\\theta)}\n",
    "$$\n",
    "\n",
    "The Cramér-Rao theorem would be true if $Cov(T, S_n) = 1$. Let's see if we can show this.\n",
    "\n",
    "First note that since $E(S_n) = 0$,\n",
    "$$\n",
    "Cov(T, S_n) = 1 ~ \\iff ~ E(TS_n) = 1\n",
    "$$\n",
    "\n",
    "We'll have to find that expected product. What we have going for us is that $T$ is unbiased. So let's use that.\n",
    "\n",
    "To simplify notation, let $\\underset{\\sim}{x} = x_1, x_2, \\ldots, x_n$ and $d\\underset{\\sim}{x}$ denote $dx_1dx_2...dx_n$. \n",
    "\n",
    "Since $T$ is unbiased,\n",
    "$$\n",
    "\\theta = E(T) = ∫_{\\underset{\\sim}{x}} T(\\underset{\\sim}{x}) \\left[\\Pi_{i=1}^{n} f_\\theta(x_i) \\right] d\\underset{\\sim}{x}\n",
    "$$\n",
    "\n",
    "Differentiate both sides with respect to $\\theta$:\n",
    "$$\n",
    "1 = \\frac{d}{d\\theta}∫_{\\underset{\\sim}{x}} T(\\underset{\\sim}{x}) \\left[\\Pi_{i=1}^{n} f_\\theta(x_i) \\right] d\\underset{\\sim}{x}\n",
    "$$\n",
    "\n",
    "Under regularity conditions that we assume are met, we can switch the derivative and integal on the right side to get\n",
    "\n",
    "\\begin{align*}\n",
    "1 &= ∫_{\\underset{\\sim}{x}} \\frac{d}{d\\theta} T(\\underset{\\sim}{x}) \\left[\\Pi_{i=1}^{n} f_\\theta(x_i) \\right] d\\underset{\\sim}{x} \\\\\n",
    "&= ∫_{\\underset{\\sim}{x}} T(\\underset{\\sim}{x}) \\frac{d}{d\\theta} \\left[\\Pi_{i=1}^{n} f_\\theta(x_i) \\right] d\\underset{\\sim}{x} \n",
    "\\end{align*}\n",
    "\n",
    "Recall that $\\ell_1(\\theta; x_k) = \\log(f_\\theta(x_k))$ and therefore \n",
    "$$\\ell_1'(\\theta; x_k) = \\frac{f_\\theta'(x_k)}{f_\\theta(x_k)}$$\n",
    "\n",
    "By the product rule for derivatives, $\\frac{d}{d\\theta} \\left[\\Pi_{i=1}^{n} f_\\theta(x_i) \\right]$ is the sum of $n$ terms, of which term $k$ is equal to\n",
    "\n",
    "\\begin{align*}\n",
    "f_\\theta'(x_k ) \\left[\\Pi_{i\\neq k} f_\\theta(x_i) \\right]\n",
    "&= \\ell_1'(\\theta; x_k) f_\\theta(x_k) \\left[\\Pi_{i\\neq k} f_\\theta(x_i) \\right] \\\\\n",
    "&= \\ell_1'(\\theta; x_k) \\left[\\Pi_{i=1}^{n} f_\\theta(x_i) \\right]\n",
    "\\end{align*}\n",
    "\n",
    "Plug this into our earlier equation to get\n",
    "$$\n",
    "1 ~ = ~ ∫_{\\underset{\\sim}{x}} T(\\underset{\\sim}{x})\\sum_{i=1}^{n} \\ell_1'(\\theta; x_i) \\left[\\Pi_{i=1}^{n} f_\\theta(x_i) \\right] d\\underset{\\sim}{x} = ∫_{\\underset{\\sim}{x}} T(\\underset{\\sim}{x})S_n(\\theta; \\underset{\\sim}{x})\\left[\\Pi_{i=1}^{n} f_\\theta(x_i) \\right] d\\underset{\\sim}{x}\n",
    "= E(T \\cdot S_n)\n",
    "$$\n",
    "just as we had hoped. This establishes the Cramér-Rao bound."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45157c73-81d7-4322-ae37-4dcce48c5c0e",
   "metadata": {},
   "source": [
    "#### Example: Efficiency and the Bernoulli MLE\n",
    "\n",
    "Let $X_1, X_2, \\ldots, X_n$ be i.i.d. Bernoulli $(p)$. In a [probability class](https://data140.org/textbook/content/chapter-20/maximum-likelihood/#maximum-likelihood-estimate-of-p-based-on-a-bernoulli-p-sample), you found the sample proportion $\\hat{p}_n$ to be the MLE of $p$. \n",
    "\n",
    "From Lecture 4, the Fisher information is $I(p) = \\displaystyle \\frac{1}{p(1-p)}$.\n",
    "       \n",
    "Fix $n$. Since $\\hat{p}_n$ is an i.i.d. sample proportion, we know that\n",
    "$E_p(\\hat{p}_n) = p$ and $Var_p(\\hat{p}_n) = \\displaystyle \\frac{p(1-p)}{n} = \\frac{1}{nI(p)}$.\n",
    "\n",
    "So $\\hat{p}_n$ is unbiased and efficient for all $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa7364-1611-4940-a647-d3c0972093d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
